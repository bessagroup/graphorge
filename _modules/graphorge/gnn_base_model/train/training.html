

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>graphorge.gnn_base_model.train.training &mdash; graphorge 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/readthedocs-custom.css?v=0fedbe66" />

  
      <script src="../../../../_static/documentation_options.js?v=8d563738"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            graphorge
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/getting_started/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/getting_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/getting_started/installation.html#package-manager">Package manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/getting_started/installation.html#from-source">From source</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/examples/example_workflow.html">Basic workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../_collections/examples/readme.html">Graphorge example directory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../_collections/examples/mechanics/gnn_shell_buckling/gnn_shell_buckling.html">A surrogate to estimate the knock-down factor of imperfect shells</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../_collections/examples/cfd/gnn_porous_medium/gnn_porous_medium.html">A surrogate to estimate the flow rate through a porous medium</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../_autosummary/graphorge.html">Code</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">License</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../license.html">MIT License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">graphorge</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">graphorge.gnn_base_model.train.training</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for graphorge.gnn_base_model.train.training</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Training of Graph Neural Network model.</span>

<span class="sd">Classes</span>
<span class="sd">-------</span>
<span class="sd">EarlyStopper</span>
<span class="sd">    Early stopping procedure (implicit regularizaton).</span>

<span class="sd">Functions</span>
<span class="sd">---------</span>
<span class="sd">train_model</span>
<span class="sd">    Training of Graph Neural Network model.</span>
<span class="sd">get_pytorch_optimizer</span>
<span class="sd">    Get PyTorch optimizer.</span>
<span class="sd">get_learning_rate_scheduler</span>
<span class="sd">    Get PyTorch optimizer learning rate scheduler.</span>
<span class="sd">save_training_state</span>
<span class="sd">    Save model and optimizer states at given training epoch.</span>
<span class="sd">load_training_state</span>
<span class="sd">    Load model and optimizer states from available training data.</span>
<span class="sd">remove_posterior_optim_state_files</span>
<span class="sd">    Delete optimizer training epoch state files posterior to given epoch.</span>
<span class="sd">save_loss_history</span>
<span class="sd">    Save training process loss history record.</span>
<span class="sd">load_loss_history</span>
<span class="sd">    Load training process loss history record.</span>
<span class="sd">load_lr_history</span>
<span class="sd">    Load training process learning rate history record.</span>
<span class="sd">seed_worker</span>
<span class="sd">    Set workers seed in PyTorch data loaders to preserve reproducibility.</span>
<span class="sd">read_loss_history_from_file</span>
<span class="sd">    Read training loss history from loss history record file.</span>
<span class="sd">read_lr_history_from_file(loss_record_path)</span>
<span class="sd">    Read training learning rate history from loss history record file.</span>
<span class="sd">write_training_summary_file    </span>
<span class="sd">    Write summary data file for model training process.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1">#</span>
<span class="c1">#                                                                       Modules</span>
<span class="c1"># =============================================================================</span>
<span class="c1"># Standard</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">datetime</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="c1"># Third-party</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch_geometric.loader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="c1"># Local</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gnn_base_model.model.gnn_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">GNNEPDBaseModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gnn_base_model.train.torch_loss</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_pytorch_loss</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gnn_base_model.predict.prediction</span><span class="w"> </span><span class="kn">import</span> <span class="n">predict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ioput.iostandard</span><span class="w"> </span><span class="kn">import</span> <span class="n">write_summary_file</span>
<span class="c1">#</span>
<span class="c1">#                                                          Authorship &amp; Credits</span>
<span class="c1"># =============================================================================</span>
<span class="n">__author__</span> <span class="o">=</span> <span class="s1">&#39;Bernardo Ferreira (bernardo_ferreira@brown.edu)&#39;</span>
<span class="n">__credits__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Bernardo Ferreira&#39;</span><span class="p">,</span> <span class="s1">&#39;Rui Barreira&#39;</span><span class="p">,</span> <span class="p">]</span>
<span class="n">__status__</span> <span class="o">=</span> <span class="s1">&#39;Planning&#39;</span>
<span class="c1"># =============================================================================</span>
<span class="c1">#</span>
<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="train_model">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.train_model.html#graphorge.gnn_base_model.train.training.train_model">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">train_model</span><span class="p">(</span><span class="n">n_max_epochs</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">model_init_args</span><span class="p">,</span> <span class="n">lr_init</span><span class="p">,</span>
                <span class="n">opt_algorithm</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">lr_scheduler_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">loss_nature</span><span class="o">=</span><span class="s1">&#39;node_features_out&#39;</span><span class="p">,</span>
                <span class="n">loss_type</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="p">{},</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">is_sampler_shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">data_loader_kwargs</span><span class="o">=</span><span class="p">{},</span>
                <span class="n">is_early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">early_stopping_kwargs</span><span class="o">=</span><span class="p">{},</span>
                <span class="n">load_model_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_every</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_loss_every</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">dataset_file_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">is_verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tqdm_flavor</span><span class="o">=</span><span class="s1">&#39;default&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Training of Graph Neural Network model.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_max_epochs : int</span>
<span class="sd">        Maximum number of training epochs.</span>
<span class="sd">    dataset : torch.utils.data.Dataset</span>
<span class="sd">        Graph Neural Network graph data set. Each sample corresponds to a</span>
<span class="sd">        torch_geometric.data.Data object describing a homogeneous graph.</span>
<span class="sd">    model_init_args : dict</span>
<span class="sd">        Graph Neural Network model class initialization parameters (check</span>
<span class="sd">        class GNNEPDBaseModel).</span>
<span class="sd">    lr_init : float</span>
<span class="sd">        Initial value optimizer learning rate. Constant learning rate value if</span>
<span class="sd">        no learning rate scheduler is specified (lr_scheduler_type=None).</span>
<span class="sd">    opt_algorithm : {&#39;adam&#39;,}, default=&#39;adam&#39;</span>
<span class="sd">        Optimization algorithm:</span>
<span class="sd">        </span>
<span class="sd">        &#39;adam&#39;  : Adam (torch.optim.Adam)</span>
<span class="sd">        </span>
<span class="sd">    lr_scheduler_type : {&#39;steplr&#39;, &#39;explr&#39;, &#39;linlr&#39;}, default=None</span>
<span class="sd">        Type of learning rate scheduler:</span>

<span class="sd">        &#39;steplr&#39;  : Step-based decay (torch.optim.lr_scheduler.SetpLR)</span>
<span class="sd">        </span>
<span class="sd">        &#39;explr&#39;   : Exponential decay (torch.optim.lr_scheduler.ExponentialLR)</span>
<span class="sd">        </span>
<span class="sd">        &#39;linlr&#39;   : Linear decay (torch.optim.lr_scheduler.LinearLR)</span>

<span class="sd">    lr_scheduler_kwargs : dict, default={}</span>
<span class="sd">        Arguments of torch.optim.lr_scheduler.LRScheduler initializer.</span>
<span class="sd">    </span>
<span class="sd">    loss_nature : {&#39;node_features_out&#39;, \</span>
<span class="sd">                   &#39;edge_features_out&#39;, \</span>
<span class="sd">                   &#39;global_features_out&#39;}, \</span>
<span class="sd">                  default=&#39;node_features_out&#39;</span>
<span class="sd">        Loss nature:</span>
<span class="sd">        </span>
<span class="sd">        &#39;node_features_out&#39; : Based on node output features</span>

<span class="sd">        &#39;edge_features_out&#39; : Based on edge output features</span>

<span class="sd">        &#39;global_features_out&#39; : Based on global output features</span>

<span class="sd">    loss_type : {&#39;mse&#39;,}, default=&#39;mse&#39;</span>
<span class="sd">        Loss function type:</span>
<span class="sd">        </span>
<span class="sd">        &#39;mse&#39;  : MSE (torch.nn.MSELoss)</span>
<span class="sd">        </span>
<span class="sd">    loss_kwargs : dict, default={}</span>
<span class="sd">        Arguments of torch.nn._Loss initializer.</span>
<span class="sd">    batch_size : int, default=1</span>
<span class="sd">        Number of samples loaded per batch.</span>
<span class="sd">    is_sampler_shuffle : bool, default=False</span>
<span class="sd">        If True, shuffles data set samples at every epoch.</span>
<span class="sd">    data_loader_kwargs : dict, default={}</span>
<span class="sd">        Additional arguments for torch_geometric.loader.dataloader.DataLoader.</span>
<span class="sd">    is_early_stopping : bool, default=False</span>
<span class="sd">        If True, then training process is halted when early stopping criterion</span>
<span class="sd">        is triggered. By default, 20% of the training data set is allocated for</span>
<span class="sd">        the underlying validation procedures.</span>
<span class="sd">    early_stopping_kwargs : dict, default={}</span>
<span class="sd">        Early stopping criterion parameters (key, str, item, value).</span>
<span class="sd">    load_model_state : {&#39;best&#39;, &#39;last&#39;, &#39;init&#39;, int, None}, default=None</span>
<span class="sd">        Load available GNN-based model state from the model</span>
<span class="sd">        directory. Data scalers are also loaded from model initialization file.</span>
<span class="sd">        Options:</span>
<span class="sd">        </span>
<span class="sd">        &#39;best&#39;      : Model state corresponding to best performance available</span>
<span class="sd">        </span>
<span class="sd">        &#39;last&#39;      : Model state corresponding to highest training epoch</span>
<span class="sd">        </span>
<span class="sd">        int         : Model state corresponding to given training epoch</span>
<span class="sd">        </span>
<span class="sd">        &#39;init&#39;      : Model state corresponding to initial state</span>
<span class="sd">        </span>
<span class="sd">        None        : Model default state file</span>

<span class="sd">    save_every : int, default=None</span>
<span class="sd">        Save Graph Neural Network model every save_every epochs. If None, then</span>
<span class="sd">        saves only last epoch and best performance states.</span>
<span class="sd">    save_loss_every : int, default=None</span>
<span class="sd">        Save loss history model every save_loss_every epochs. If None, then</span>
<span class="sd">        saves loss history only after the last epoch.</span>
<span class="sd">    dataset_file_path : str, default=None</span>
<span class="sd">        Graph Neural Network graph data set file path if such file exists. Only</span>
<span class="sd">        used for output purposes.</span>
<span class="sd">    device_type : {&#39;cpu&#39;, &#39;cuda&#39;}, default=&#39;cpu&#39;</span>
<span class="sd">        Type of device on which torch.Tensor is allocated.</span>
<span class="sd">    seed : int, default=None</span>
<span class="sd">        Seed used to initialize the random number generators of Python and</span>
<span class="sd">        other libraries (e.g., NumPy, PyTorch) for all devices to preserve</span>
<span class="sd">        reproducibility. Does also set workers seed in PyTorch data loaders.</span>
<span class="sd">    is_verbose : bool, default=False</span>
<span class="sd">        If True, enable verbose output.</span>
<span class="sd">    tqdm_flavor : {&#39;default&#39;, &#39;notebook&#39;}, default=&#39;default&#39;</span>
<span class="sd">        Type of tqdm progress bar to use when is_verbose=True.</span>
<span class="sd">        </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    model : torch.nn.Module</span>
<span class="sd">        Graph Neural Network model.</span>
<span class="sd">    best_loss : float</span>
<span class="sd">        Best loss during training process.</span>
<span class="sd">    best_training_epoch : int</span>
<span class="sd">        Training epoch corresponding to best loss during training process.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Import tqdm</span>
    <span class="k">if</span> <span class="n">tqdm_flavor</span> <span class="o">==</span> <span class="s1">&#39;notebook&#39;</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">tqdm.notebook</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set random number generators initialization for reproducibility</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
            <span class="n">device_name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device_name</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Setting device: </span><span class="si">{</span><span class="n">device_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="n">start_time_sec</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Graph Neural Network model training&#39;</span>
              <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">-----------------------------------&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Initialize Graph Neural Network model state</span>
    <span class="k">if</span> <span class="n">load_model_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Initializing model...&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize Graph Neural Network model</span>
        <span class="c1"># (includes loading of data scalers)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">GNNEPDBaseModel</span><span class="o">.</span><span class="n">init_model_from_file</span><span class="p">(</span>
            <span class="n">model_init_args</span><span class="p">[</span><span class="s1">&#39;model_directory&#39;</span><span class="p">])</span>
        <span class="c1"># Set model device</span>
        <span class="n">model</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Get model input and output features normalization</span>
        <span class="n">is_model_in_normalized</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">is_model_in_normalized</span>
        <span class="n">is_model_out_normalized</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">is_model_out_normalized</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Loading model state...&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Load Graph Neural Network model state</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_model_state</span><span class="p">(</span><span class="n">load_model_state</span><span class="o">=</span><span class="n">load_model_state</span><span class="p">,</span>
                                   <span class="n">is_remove_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Initializing model...&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize Graph Neural Network model</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">GNNEPDBaseModel</span><span class="p">(</span><span class="o">**</span><span class="n">model_init_args</span><span class="p">)</span>    
        <span class="c1"># Set model device</span>
        <span class="n">model</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Get model input and output features normalization</span>
        <span class="n">is_model_in_normalized</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">is_model_in_normalized</span>
        <span class="n">is_model_out_normalized</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">is_model_out_normalized</span>
        <span class="c1"># Fit model data scalers  </span>
        <span class="k">if</span> <span class="n">is_model_in_normalized</span> <span class="ow">or</span> <span class="n">is_model_out_normalized</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">fit_data_scalers</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">is_verbose</span><span class="o">=</span><span class="n">is_verbose</span><span class="p">,</span>
                                   <span class="n">tqdm_flavor</span><span class="o">=</span><span class="n">tqdm_flavor</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  </span>
    <span class="c1"># Save model initial state</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_model_init_state</span><span class="p">()</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Get model parameters</span>
    <span class="n">model_parameters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  </span>
    <span class="c1"># Move model to device</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Set model in training mode</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Initialize learning rate</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr_init</span>
    <span class="c1"># Set optimizer    </span>
    <span class="k">if</span> <span class="n">opt_algorithm</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
        <span class="c1"># Initialize optimizer, specifying the model (and submodels) parameters</span>
        <span class="c1"># that should be optimized. By default, model parameters gradient flag</span>
        <span class="c1"># is set to True, meaning that gradients with respect to the parameters</span>
        <span class="c1"># are required (operations on the parameters are recorded for automatic</span>
        <span class="c1"># differentiation)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_parameters</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown optimization algorithm&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Initialize learning rate scheduler</span>
    <span class="n">is_lr_scheduler</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">lr_scheduler_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">is_lr_scheduler</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_learning_rate_scheduler</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler_type</span><span class="o">=</span><span class="n">lr_scheduler_type</span><span class="p">,</span>
            <span class="o">**</span><span class="n">lr_scheduler_kwargs</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Initialize loss function</span>
    <span class="n">loss_function</span> <span class="o">=</span> <span class="n">get_pytorch_loss</span><span class="p">(</span><span class="n">loss_type</span><span class="p">,</span> <span class="o">**</span><span class="n">loss_kwargs</span><span class="p">)</span>
    <span class="c1"># Initialize loss and learning rate histories (per epoch)</span>
    <span class="n">loss_history_epochs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lr_history_epochs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Initialize loss and learning rate histories (per training step)</span>
    <span class="n">loss_history_steps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lr_history_steps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Initialize training flag</span>
    <span class="n">is_keep_training</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># Initialize number of training epochs</span>
    <span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Initialize number of training steps</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Initialize validation loss history</span>
    <span class="n">validation_loss_history</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Initialize early stopping criterion</span>
    <span class="k">if</span> <span class="n">is_early_stopping</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Initializing early stopping criterion...&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize early stopping criterion</span>
        <span class="n">early_stopper</span> <span class="o">=</span> <span class="n">EarlyStopper</span><span class="p">(</span><span class="o">**</span><span class="n">early_stopping_kwargs</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize early stopping flag</span>
        <span class="n">is_stop_training</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Training data set size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set data loader</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch_geometric</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
            <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span> <span class="o">**</span><span class="n">data_loader_kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch_geometric</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">dataloader</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">is_sampler_shuffle</span><span class="p">,</span>
            <span class="o">**</span><span class="n">data_loader_kwargs</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
        <span class="n">input_normalization_str</span> <span class="o">=</span> <span class="s1">&#39;Yes&#39;</span> <span class="k">if</span> <span class="n">is_model_in_normalized</span> <span class="k">else</span> <span class="s1">&#39;No&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Input data normalization: </span><span class="si">{</span><span class="n">input_normalization_str</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">output_normalization_str</span> <span class="o">=</span> <span class="s1">&#39;Yes&#39;</span> <span class="k">if</span> <span class="n">is_model_out_normalized</span> <span class="k">else</span> <span class="s1">&#39;No&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Output data normalization: </span><span class="si">{</span><span class="n">output_normalization_str</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&gt; Starting training process...</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">if</span> <span class="n">tqdm_flavor</span> <span class="o">==</span> <span class="s1">&#39;notebook&#39;</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">tqdm.notebook</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
        <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">n_max_epochs</span><span class="p">,</span> 
                    <span class="n">mininterval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">maxinterval</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
                    <span class="n">miniters</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;&gt; Epochs: &#39;</span><span class="p">,</span>
                    <span class="n">unit</span><span class="o">=</span><span class="s1">&#39; epoch&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Loop over training iterations</span>
    <span class="k">while</span> <span class="n">is_keep_training</span><span class="p">:</span>
        <span class="c1"># Store epoch initial training step</span>
        <span class="n">epoch_init_step</span> <span class="o">=</span> <span class="n">step</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Loop over graph batches. A graph batch is a data object describing a</span>
        <span class="c1"># batch of graphs as one large (disconnected) graph.</span>
        <span class="k">for</span> <span class="n">pyg_graph</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">data_loader</span><span class="p">,</span>
                              <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">mininterval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">maxinterval</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
                              <span class="n">miniters</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                              <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;&gt; Steps: &#39;</span><span class="p">,</span>
                              <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">is_verbose</span><span class="p">,</span>
                              <span class="n">unit</span><span class="o">=</span><span class="s1">&#39; step&#39;</span><span class="p">):</span>
            <span class="c1"># Move graph sample to device</span>
            <span class="n">pyg_graph</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Get batch node assignment vector</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">batch_vector</span> <span class="o">=</span> <span class="n">pyg_graph</span><span class="o">.</span><span class="n">batch</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">batch_vector</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Get input features from input graph</span>
            <span class="n">node_features_in</span><span class="p">,</span> <span class="n">edge_features_in</span><span class="p">,</span> <span class="n">global_features_in</span><span class="p">,</span> \
                <span class="n">edges_indexes</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_input_features_from_graph</span><span class="p">(</span>
                    <span class="n">pyg_graph</span><span class="p">,</span> <span class="n">is_normalized</span><span class="o">=</span><span class="n">is_model_in_normalized</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Get node output features ground-truth</span>
            <span class="n">node_targets</span><span class="p">,</span> <span class="n">edge_targets</span><span class="p">,</span> <span class="n">global_targets</span> <span class="o">=</span> \
                <span class="n">model</span><span class="o">.</span><span class="n">get_output_features_from_graph</span><span class="p">(</span>
                    <span class="n">pyg_graph</span><span class="p">,</span> <span class="n">is_normalized</span><span class="o">=</span><span class="n">is_model_out_normalized</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Compute output features predictions (forward propagation).</span>
            <span class="c1"># During the foward pass, PyTorch creates a computation graph for</span>
            <span class="c1"># the tensors that require gradients (gradient flag set to True) to</span>
            <span class="c1"># keep track of the operations on these tensors, i.e., the model</span>
            <span class="c1"># parameters. In addition, PyTorch additionally stores the</span>
            <span class="c1"># corresponding &#39;gradient functions&#39; (mathematical operator) of the</span>
            <span class="c1"># executed operations to the output tensor, stored in the .grad_fn</span>
            <span class="c1"># attribute of the corresponding tensors. Tensor.grad_fn is set to</span>
            <span class="c1"># None for tensors corresponding to leaf-nodes of the computation</span>
            <span class="c1"># graph or for tensors with the gradient flag set to False.</span>
            <span class="k">if</span> <span class="n">loss_nature</span> <span class="o">==</span> <span class="s1">&#39;node_features_out&#39;</span><span class="p">:</span>
                <span class="c1"># Get node output features</span>
                <span class="n">node_features_out</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
                    <span class="n">node_features_in</span><span class="o">=</span><span class="n">node_features_in</span><span class="p">,</span>
                    <span class="n">edge_features_in</span><span class="o">=</span><span class="n">edge_features_in</span><span class="p">,</span>
                    <span class="n">global_features_in</span><span class="o">=</span><span class="n">global_features_in</span><span class="p">,</span>
                    <span class="n">edges_indexes</span><span class="o">=</span><span class="n">edges_indexes</span><span class="p">,</span>
                    <span class="n">batch_vector</span><span class="o">=</span><span class="n">batch_vector</span><span class="p">)</span>
                <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
                <span class="c1"># Compute loss</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">node_features_out</span><span class="p">,</span> <span class="n">node_targets</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="k">elif</span> <span class="n">loss_nature</span> <span class="o">==</span> <span class="s1">&#39;edge_features_out&#39;</span><span class="p">:</span>
                <span class="c1"># Get edge output features</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">edge_features_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
                    <span class="n">node_features_in</span><span class="o">=</span><span class="n">node_features_in</span><span class="p">,</span>
                    <span class="n">edge_features_in</span><span class="o">=</span><span class="n">edge_features_in</span><span class="p">,</span>
                    <span class="n">global_features_in</span><span class="o">=</span><span class="n">global_features_in</span><span class="p">,</span>
                    <span class="n">edges_indexes</span><span class="o">=</span><span class="n">edges_indexes</span><span class="p">,</span>
                    <span class="n">batch_vector</span><span class="o">=</span><span class="n">batch_vector</span><span class="p">)</span>
                <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
                <span class="c1"># Compute loss</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">edge_features_out</span><span class="p">,</span> <span class="n">edge_targets</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="k">elif</span> <span class="n">loss_nature</span> <span class="o">==</span> <span class="s1">&#39;global_features_out&#39;</span><span class="p">:</span>
                <span class="c1"># Get global output features</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">global_features_out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
                    <span class="n">node_features_in</span><span class="o">=</span><span class="n">node_features_in</span><span class="p">,</span>
                    <span class="n">edge_features_in</span><span class="o">=</span><span class="n">edge_features_in</span><span class="p">,</span>
                    <span class="n">global_features_in</span><span class="o">=</span><span class="n">global_features_in</span><span class="p">,</span>
                    <span class="n">edges_indexes</span><span class="o">=</span><span class="n">edges_indexes</span><span class="p">,</span>
                    <span class="n">batch_vector</span><span class="o">=</span><span class="n">batch_vector</span><span class="p">)</span>
                <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
                <span class="c1"># Compute loss</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">global_features_out</span><span class="p">,</span> <span class="n">global_targets</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown loss nature.&#39;</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Initialize gradients (set to zero)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># Compute gradients with respect to model parameters (backward</span>
            <span class="c1"># propagation). PyTorch backpropagates recursively through the</span>
            <span class="c1"># computation graph of loss and computes the gradients with respect</span>
            <span class="c1"># to the model parameters. On each Tensor, PyTorch computes the</span>
            <span class="c1"># local gradients using the previously stored .grad_fn mathematical</span>
            <span class="c1"># operators and combines them with the incoming gradients to</span>
            <span class="c1"># compute the complete gradient (i.e., building the</span>
            <span class="c1"># differentiation chain rule). The backward propagation recursive</span>
            <span class="c1"># path stops when a leaf-node is reached (e.g., a model parameter),</span>
            <span class="c1"># where .grad_fn is set to None. Gradients are cumulatively stored</span>
            <span class="c1"># in the .grad attribute of the corresponding tensors</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># Perform optimization step. Gradients are stored in the .grad</span>
            <span class="c1"># attribute of model parameters</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Save training step loss and learning rate</span>
            <span class="n">loss_history_steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">is_lr_scheduler</span><span class="p">:</span>
                <span class="n">lr_history_steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lr_history_steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr_init</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Increment training step counter</span>
            <span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Update optimizer learning rate</span>
        <span class="k">if</span> <span class="n">is_lr_scheduler</span><span class="p">:</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Save training epoch loss (epoch average loss value)</span>
        <span class="n">epoch_avg_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss_history_steps</span><span class="p">[</span><span class="n">epoch_init_step</span><span class="p">:])</span>
        <span class="n">loss_history_epochs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_avg_loss</span><span class="p">)</span>
        <span class="c1"># Save training epoch learning rate (epoch last value)</span>
        <span class="k">if</span> <span class="n">is_lr_scheduler</span><span class="p">:</span>
            <span class="n">lr_history_epochs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lr_history_epochs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr_init</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Save model and optimizer current states</span>
        <span class="k">if</span> <span class="n">save_every</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">save_training_state</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">if</span> <span class="n">save_loss_every</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">save_loss_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Get validation loss history</span>
            <span class="k">if</span> <span class="n">is_early_stopping</span><span class="p">:</span>
                <span class="n">validation_loss_history</span> <span class="o">=</span> \
                    <span class="n">early_stopper</span><span class="o">.</span><span class="n">get_validation_loss_history</span><span class="p">()</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Save loss and learning rate histories</span>
            <span class="n">save_loss_history</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_max_epochs</span><span class="p">,</span> <span class="n">loss_nature</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">,</span>
                              <span class="n">loss_history_epochs</span><span class="p">,</span>
                              <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="n">lr_scheduler_type</span><span class="p">,</span>
                              <span class="n">lr_history_epochs</span><span class="o">=</span><span class="n">lr_history_epochs</span><span class="p">,</span>
                              <span class="n">validation_loss_history</span><span class="o">=</span><span class="n">validation_loss_history</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Save model and optimizer best performance state corresponding to</span>
        <span class="c1"># minimum training loss</span>
        <span class="k">if</span> <span class="n">epoch_avg_loss</span> <span class="o">&lt;=</span> <span class="nb">min</span><span class="p">(</span><span class="n">loss_history_epochs</span><span class="p">):</span>
            <span class="n">save_training_state</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">is_best_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Check early stopping criterion</span>
        <span class="k">if</span> <span class="n">is_early_stopping</span><span class="p">:</span>
            <span class="c1"># Evaluate early stopping criterion</span>
            <span class="k">if</span> <span class="n">early_stopper</span><span class="o">.</span><span class="n">is_evaluate_criterion</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
                <span class="n">is_stop_training</span> <span class="o">=</span> <span class="n">early_stopper</span><span class="o">.</span><span class="n">evaluate_criterion</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_nature</span><span class="o">=</span><span class="n">loss_nature</span><span class="p">,</span>
                    <span class="n">loss_type</span><span class="o">=</span><span class="n">loss_type</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="n">loss_kwargs</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">)</span>
            <span class="c1"># If early stopping is triggered, save model and optimizer best</span>
            <span class="c1"># performance corresponding to early stopping criterion</span>
            <span class="k">if</span> <span class="n">is_stop_training</span><span class="p">:</span>
                <span class="c1"># Load best performance model and optimizer states</span>
                <span class="n">best_epoch</span> <span class="o">=</span> <span class="n">early_stopper</span><span class="o">.</span><span class="n">load_best_performance_state</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
                <span class="c1"># Save model and optimizer best performance states</span>
                <span class="n">save_training_state</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">best_epoch</span><span class="p">,</span>
                                    <span class="n">is_best_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Increment epoch counter</span>
        <span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Update progress bar</span>
        <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Check training process flow</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">n_max_epochs</span><span class="p">:</span>
            <span class="c1"># Completed maximum number of epochs</span>
            <span class="n">is_keep_training</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>
        <span class="k">elif</span> <span class="n">is_early_stopping</span> <span class="ow">and</span> <span class="n">is_stop_training</span><span class="p">:</span>
            <span class="c1"># Early stopping criterion triggered</span>
            <span class="n">is_keep_training</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>            

    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_early_stopping</span> <span class="ow">and</span> <span class="n">is_stop_training</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&gt; Early stopping has been triggered!&#39;</span><span class="p">,</span>
                  <span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&gt; Finished training process!&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&gt; Finished training process!&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Get validation loss history</span>
    <span class="k">if</span> <span class="n">is_early_stopping</span><span class="p">:</span>
        <span class="n">validation_loss_history</span> <span class="o">=</span> \
            <span class="n">early_stopper</span><span class="o">.</span><span class="n">get_validation_loss_history</span><span class="p">()</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Save model and optimizer final states</span>
    <span class="n">save_training_state</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
    <span class="c1"># Save loss and learning rate histories</span>
    <span class="n">save_loss_history</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_max_epochs</span><span class="p">,</span> <span class="n">loss_nature</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">,</span>
                      <span class="n">loss_history_epochs</span><span class="p">,</span> <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="n">lr_scheduler_type</span><span class="p">,</span>
                      <span class="n">lr_history_epochs</span><span class="o">=</span><span class="n">lr_history_epochs</span><span class="p">,</span>
                      <span class="n">validation_loss_history</span><span class="o">=</span><span class="n">validation_loss_history</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Get best loss and corresponding training epoch</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">loss_history_epochs</span><span class="p">))</span>
    <span class="n">best_training_epoch</span> <span class="o">=</span> <span class="n">loss_history_epochs</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">best_loss</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_model_out_normalized</span><span class="p">:</span>
            <span class="n">min_loss_str</span> <span class="o">=</span> <span class="s1">&#39;Minimum training loss (normalized)&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_loss_str</span> <span class="o">=</span> <span class="s1">&#39;Minimum training loss&#39;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&gt; </span><span class="si">{</span><span class="n">min_loss_str</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">best_loss</span><span class="si">:</span><span class="s1">.8e</span><span class="si">}</span><span class="s1"> | &#39;</span>
              <span class="sa">f</span><span class="s1">&#39;Epoch: </span><span class="si">{</span><span class="n">best_training_epoch</span><span class="si">:</span><span class="s1">d</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Compute total training time and average training time per epoch</span>
    <span class="n">total_time_sec</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time_sec</span>
    <span class="n">avg_time_epoch</span> <span class="o">=</span> <span class="n">total_time_sec</span><span class="o">/</span><span class="n">epoch</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">if</span> <span class="n">is_verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Model directory: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">total_time_sec</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time_sec</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&gt; Total training time: &#39;</span>
              <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">total_time_sec</span><span class="p">)))</span><span class="si">}</span><span class="s1"> | &#39;</span>
              <span class="sa">f</span><span class="s1">&#39;Avg. training time per epoch: &#39;</span>
              <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">avg_time_epoch</span><span class="p">)))</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Write summary data file for model training process</span>
    <span class="n">write_training_summary_file</span><span class="p">(</span>
        <span class="n">device_type</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span> <span class="n">load_model_state</span><span class="p">,</span>
        <span class="n">n_max_epochs</span><span class="p">,</span> <span class="n">is_model_in_normalized</span><span class="p">,</span> <span class="n">is_model_out_normalized</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">is_sampler_shuffle</span><span class="p">,</span> <span class="n">loss_nature</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="p">,</span>
        <span class="n">opt_algorithm</span><span class="p">,</span> <span class="n">lr_init</span><span class="p">,</span> <span class="n">lr_scheduler_type</span><span class="p">,</span> <span class="n">lr_scheduler_kwargs</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span>
        <span class="n">dataset_file_path</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">best_training_epoch</span><span class="p">,</span>
        <span class="n">total_time_sec</span><span class="p">,</span> <span class="n">avg_time_epoch</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">best_training_epoch</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="get_pytorch_optimizer">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.get_pytorch_optimizer.html#graphorge.gnn_base_model.train.training.get_pytorch_optimizer">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_pytorch_optimizer</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get PyTorch optimizer.</span>
<span class="sd">   </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    algorithm : {&#39;adam&#39;,}</span>
<span class="sd">        Optimization algorithm:</span>
<span class="sd">        </span>
<span class="sd">        &#39;adam&#39;  : Adam (torch.optim.Adam)</span>
<span class="sd">        </span>
<span class="sd">    params : list</span>
<span class="sd">        List of parameters (torch.Tensors) to optimize or list of dicts</span>
<span class="sd">        defining parameter groups.</span>
<span class="sd">    **kwargs</span>
<span class="sd">        Arguments of torch.optim.Optimizer initializer.</span>
<span class="sd">        </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    optimizer : torch.optim.Optimizer</span>
<span class="sd">        PyTorch optimizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">algorithm</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown or unavailable PyTorch optimizer.&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">optimizer</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="get_learning_rate_scheduler">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.get_learning_rate_scheduler.html#graphorge.gnn_base_model.train.training.get_learning_rate_scheduler">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">get_learning_rate_scheduler</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler_type</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get PyTorch optimizer learning rate scheduler.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    optimizer : torch.optim.Optimizer</span>
<span class="sd">        PyTorch optimizer.</span>
<span class="sd">    lr_scheduler_type : {&#39;steplr&#39;, &#39;explr&#39;, &#39;linlr&#39;}</span>
<span class="sd">        Type of learning rate scheduler:</span>

<span class="sd">        &#39;steplr&#39;  : Step-based decay (torch.optim.lr_scheduler.SetpLR)</span>
<span class="sd">        </span>
<span class="sd">        &#39;explr&#39;   : Exponential decay (torch.optim.lr_scheduler.ExponentialLR)</span>
<span class="sd">        </span>
<span class="sd">        &#39;linlr&#39;   : Linear decay (torch.optim.lr_scheduler.LinearLR)</span>

<span class="sd">    **kwargs</span>
<span class="sd">        Arguments of torch.optim.lr_scheduler.LRScheduler initializer.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    scheduler : torch.optim.lr_scheduler.LRScheduler</span>
<span class="sd">        PyTorch optimizer learning rate scheduler.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">scheduler_type</span> <span class="o">==</span> <span class="s1">&#39;steplr&#39;</span><span class="p">:</span>
        <span class="c1"># Check scheduler mandatory parameters</span>
        <span class="k">if</span> <span class="s1">&#39;step_size&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The parameter </span><span class="se">\&#39;</span><span class="s1">step_size</span><span class="se">\&#39;</span><span class="s1"> needs to be &#39;</span>
                               <span class="s1">&#39;provided to initialize step-based decay &#39;</span>
                               <span class="s1">&#39;learning rate scheduler.&#39;</span><span class="p">)</span>
        <span class="c1"># Initialize scheduler</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">elif</span> <span class="n">scheduler_type</span> <span class="o">==</span> <span class="s1">&#39;explr&#39;</span><span class="p">:</span>
        <span class="c1"># Check scheduler mandatory parameters</span>
        <span class="k">if</span> <span class="s1">&#39;gamma&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The parameter </span><span class="se">\&#39;</span><span class="s1">gamma</span><span class="se">\&#39;</span><span class="s1"> needs to be &#39;</span>
                               <span class="s1">&#39;provided to initialize exponential decay &#39;</span>
                               <span class="s1">&#39;learning rate scheduler.&#39;</span><span class="p">)</span>
        <span class="c1"># Initialize scheduler</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">elif</span> <span class="n">scheduler_type</span> <span class="o">==</span> <span class="s1">&#39;linlr&#39;</span><span class="p">:</span>
        <span class="c1"># Initialize scheduler</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LinearLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown or unavailable PyTorch optimizer &#39;</span>
                           <span class="s1">&#39;learning rate scheduler.&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">scheduler</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="save_training_state">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.save_training_state.html#graphorge.gnn_base_model.train.training.save_training_state">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">save_training_state</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">is_best_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_remove_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save model and optimizer states at given training epoch.</span>
<span class="sd">    </span>
<span class="sd">    Material patch model state file is stored in model_directory under the</span>
<span class="sd">    name &lt; model_name &gt;.pt or &lt; model_name &gt;-&lt; epoch &gt;.pt if epoch is known.</span>
<span class="sd">    </span>
<span class="sd">    Material patch model state file corresponding to the best performance</span>
<span class="sd">    is stored in model_directory under the name &lt; model_name &gt;-best.pt or</span>
<span class="sd">    &lt; model_name &gt;-&lt; epoch &gt;-best.pt if epoch is known.</span>
<span class="sd">        </span>
<span class="sd">    Optimizer state file is stored in model_directory under the name</span>
<span class="sd">    &lt; model_name &gt;_optim-&lt; epoch &gt;.pt.</span>
<span class="sd">    </span>
<span class="sd">    Optimizer state file corresponding to the best performance is stored in</span>
<span class="sd">    model_directory under the name &lt; model_name &gt;_optim-best.pt or</span>
<span class="sd">    &lt; model_name &gt;_optim-&lt; epoch &gt;-best.pt if epoch is known.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model : torch.nn.Module</span>
<span class="sd">        Model.</span>
<span class="sd">    optimizer : torch.optim.Optimizer</span>
<span class="sd">        PyTorch optimizer.</span>
<span class="sd">    epoch : int, default=None</span>
<span class="sd">        Training epoch.</span>
<span class="sd">    is_best_state : bool, default=False</span>
<span class="sd">        If True, save material patch model state file corresponding to the best</span>
<span class="sd">        performance instead of regular state file.</span>
<span class="sd">    is_remove_posterior : bool, default=True</span>
<span class="sd">        Remove material patch model and optimizer state files corresponding to</span>
<span class="sd">        training epochs posterior to the saved state file. Effective only if</span>
<span class="sd">        saved epoch is known.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Save model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_model_state</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">is_best_state</span><span class="o">=</span><span class="n">is_best_state</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set optimizer state file</span>
    <span class="n">optimizer_state_file</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model_name</span> <span class="o">+</span> <span class="s1">&#39;_optim&#39;</span>
    <span class="c1"># Append epoch</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">optimizer_state_file</span> <span class="o">+=</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set optimizer state file corresponding to best performance</span>
    <span class="k">if</span> <span class="n">is_best_state</span><span class="p">:</span>
        <span class="c1"># Append best performance</span>
        <span class="n">optimizer_state_file</span> <span class="o">+=</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="s1">&#39;best&#39;</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Get optimizer state files in material patch model directory</span>
        <span class="n">directory_list</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">)</span>
        <span class="c1"># Loop over files in material patch model directory</span>
        <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">directory_list</span><span class="p">:</span>
            <span class="c1"># Check if file is optimizer epoch best state file</span>
            <span class="n">is_best_state_file</span> <span class="o">=</span> \
                <span class="nb">bool</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;^&#39;</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">model_name</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;_optim&#39;</span>
                               <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;-?[0-9]*&#39;</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;-best&#39;</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;\.pt&#39;</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>
            <span class="c1"># Delete state file</span>
            <span class="k">if</span> <span class="n">is_best_state_file</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>      
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set optimizer state file path</span>
    <span class="n">optimizer_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span>
                                  <span class="n">optimizer_state_file</span> <span class="o">+</span> <span class="s1">&#39;.pt&#39;</span><span class="p">)</span>
    <span class="c1"># Save optimizer state</span>
    <span class="n">optimizer_state</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                           <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">optimizer_state</span><span class="p">,</span> <span class="n">optimizer_path</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Delete model and optimizer epoch state files posterior to saved epoch</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_remove_posterior</span><span class="p">:</span>
        <span class="n">remove_posterior_optim_state_files</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="load_training_state">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.load_training_state.html#graphorge.gnn_base_model.train.training.load_training_state">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">load_training_state</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">opt_algorithm</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span>
                        <span class="n">load_model_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_remove_posterior</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load model and optimizer states from available training data.</span>
<span class="sd">    </span>
<span class="sd">    Material patch model state file is stored in model_directory under the</span>
<span class="sd">    name &lt; model_name &gt;.pt, &lt; model_name &gt;-&lt; epoch &gt;.pt,</span>
<span class="sd">    &lt; model_name &gt;-best.pt or &lt; model_name &gt;-&lt; epoch &gt;-best.pt.</span>

<span class="sd">    Optimizer state file is stored in model_directory under the name</span>
<span class="sd">    &lt; model_name &gt;_optim.pt or &lt; model_name &gt;_optim-&lt; epoch &gt;.pt.</span>
<span class="sd">    </span>
<span class="sd">    Both model and optimizer are updated &#39;in-place&#39; with loaded state data.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model : torch.nn.Module</span>
<span class="sd">        Model.</span>
<span class="sd">    opt_algorithm : {&#39;adam&#39;,}, default=&#39;adam&#39;</span>
<span class="sd">        Optimization algorithm:</span>
<span class="sd">        </span>
<span class="sd">        &#39;adam&#39;  : Adam (torch.optim.Adam)</span>

<span class="sd">    optimizer : torch.optim.Optimizer</span>
<span class="sd">        PyTorch optimizer.</span>
<span class="sd">    load_model_state : {&#39;best&#39;, &#39;last&#39;, int, None}, default=None</span>
<span class="sd">        Load available Graph Neural Network model state from the model</span>
<span class="sd">        directory. Options:</span>
<span class="sd">        </span>
<span class="sd">        &#39;best&#39;      : Model state corresponding to best performance available</span>
<span class="sd">        </span>
<span class="sd">        &#39;last&#39;      : Model state corresponding to highest training epoch</span>
<span class="sd">        </span>
<span class="sd">        int         : Model state corresponding to given training epoch</span>
<span class="sd">        </span>
<span class="sd">        None        : Model default state file</span>

<span class="sd">    is_remove_posterior : bool, default=True</span>
<span class="sd">        Remove material patch model state files corresponding to training</span>
<span class="sd">        epochs posterior to the loaded state file. Effective only if</span>
<span class="sd">        loaded training epoch is known.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loaded_epoch : int</span>
<span class="sd">        Training epoch corresponding to loaded state data. Defaults to 0 if</span>
<span class="sd">        training epoch is unknown.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load model state        </span>
    <span class="n">loaded_epoch</span> <span class="o">=</span> \
        <span class="n">model</span><span class="o">.</span><span class="n">load_model_state</span><span class="p">(</span><span class="n">load_model_state</span><span class="o">=</span><span class="n">load_model_state</span><span class="p">,</span>
                               <span class="n">is_remove_posterior</span><span class="o">=</span><span class="n">is_remove_posterior</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set optimizer state file</span>
    <span class="n">optimizer_state_file</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model_name</span> <span class="o">+</span> <span class="s1">&#39;_optim&#39;</span>
    <span class="c1"># Append epoch</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loaded_epoch</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">optimizer_state_file</span> <span class="o">+=</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">loaded_epoch</span><span class="p">)</span>
    <span class="c1"># Append best performance</span>
    <span class="k">if</span> <span class="n">load_model_state</span> <span class="o">==</span> <span class="s1">&#39;best&#39;</span><span class="p">:</span>
        <span class="n">optimizer_state_file</span> <span class="o">+=</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="s1">&#39;best&#39;</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set optimizer state file path</span>
    <span class="n">optimizer_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span>
                                  <span class="n">optimizer_state_file</span> <span class="o">+</span> <span class="s1">&#39;.pt&#39;</span><span class="p">)</span>
    <span class="c1"># Load optimizer state</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">optimizer_path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Optimizer state file has not been found:</span><span class="se">\n\n</span><span class="s1">&#39;</span>
                           <span class="o">+</span> <span class="n">optimizer_path</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Initialize optimizer</span>
        <span class="k">if</span> <span class="n">opt_algorithm</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(</span><span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown optimization algorithm&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Load optimizer state</span>
        <span class="n">optimizer_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">optimizer_path</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Set loaded optimizer state</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">optimizer_state</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">])</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Delete optimizer epoch state files posterior to loaded epoch</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loaded_epoch</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_remove_posterior</span><span class="p">:</span>
            <span class="n">remove_posterior_optim_state_files</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loaded_epoch</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set loaded epoch to 0 if unknown from state file</span>
    <span class="k">if</span> <span class="n">loaded_epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loaded_epoch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">loaded_epoch</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="remove_posterior_optim_state_files">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.remove_posterior_optim_state_files.html#graphorge.gnn_base_model.train.training.remove_posterior_optim_state_files">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">remove_posterior_optim_state_files</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Delete optimizer training epoch state files posterior to given epoch.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model : torch.nn.Module</span>
<span class="sd">        Model.</span>
<span class="sd">    epoch : int</span>
<span class="sd">        Training epoch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get files in material patch model directory</span>
    <span class="n">directory_list</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">)</span>
    <span class="c1"># Loop over files in material patch model directory</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">directory_list</span><span class="p">:</span>
        <span class="c1"># Check if file is optimizer epoch state file</span>
        <span class="n">is_state_file</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;^&#39;</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">model_name</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;_optim&#39;</span>
                             <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;-[0-9]+&#39;</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39;\.pt&#39;</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Delete optimizer epoch state file posterior to given epoch</span>
        <span class="k">if</span> <span class="n">is_state_file</span><span class="p">:</span>
            <span class="c1"># Get optimizer state epoch</span>
            <span class="n">file_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">filename</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Delete optimizer epoch state file</span>
            <span class="k">if</span> <span class="n">file_epoch</span> <span class="o">&gt;</span> <span class="n">epoch</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="save_loss_history">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.save_loss_history.html#graphorge.gnn_base_model.train.training.save_loss_history">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">save_loss_history</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n_max_epochs</span><span class="p">,</span> <span class="n">loss_nature</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">,</span>
                      <span class="n">training_loss_history</span><span class="p">,</span> <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">lr_history_epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validation_loss_history</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Save training process loss history record.</span>
<span class="sd">    </span>
<span class="sd">    Loss history record file is stored in model_directory under the name</span>
<span class="sd">    loss_history_record.pkl.</span>

<span class="sd">    Overwrites existing loss history record file.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model : torch.nn.Module</span>
<span class="sd">        Model.</span>
<span class="sd">    n_max_epochs : int</span>
<span class="sd">        Maximum number of epochs of training process.</span>
<span class="sd">    loss_nature : str</span>
<span class="sd">        Loss nature.</span>
<span class="sd">    loss_type : str</span>
<span class="sd">        Loss function type.</span>
<span class="sd">    training_loss_history : list[float]</span>
<span class="sd">        Training process training loss history (per epoch).</span>
<span class="sd">    lr_scheduler_type : {&#39;steplr&#39;, &#39;explr&#39;, &#39;linlr&#39;}, default=None</span>
<span class="sd">        Type of learning rate scheduler.</span>
<span class="sd">    lr_history_epochs : list[float], default=None</span>
<span class="sd">        Training process learning rate history (per epoch).</span>
<span class="sd">    validation_loss_history : list[float], default=None</span>
<span class="sd">        Training process validation loss history (e.g., early stopping</span>
<span class="sd">        criterion).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set loss history record file path</span>
    <span class="n">loss_record_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span>
                                    <span class="s1">&#39;loss_history_record&#39;</span> <span class="o">+</span> <span class="s1">&#39;.pkl&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Build training loss history record</span>
    <span class="n">loss_history_record</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;n_max_epochs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_max_epochs</span><span class="p">)</span>
    <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;loss_nature&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">loss_nature</span><span class="p">)</span>
    <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;loss_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">loss_type</span><span class="p">)</span>
    <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;training_loss_history&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">training_loss_history</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Store learning rate history record</span>
    <span class="k">if</span> <span class="n">lr_scheduler_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;lr_scheduler_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">lr_scheduler_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;lr_scheduler_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">lr_history_epochs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;lr_history_epochs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lr_history_epochs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;lr_history_epochs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Store validation loss history</span>
    <span class="k">if</span> <span class="n">validation_loss_history</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;validation_loss_history&#39;</span><span class="p">]</span> <span class="o">=</span> \
            <span class="nb">list</span><span class="p">(</span><span class="n">validation_loss_history</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;validation_loss_history&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Save loss history record</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">loss_record_file</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">loss_history_record</span><span class="p">,</span> <span class="n">loss_record_file</span><span class="p">)</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="load_loss_history">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.load_loss_history.html#graphorge.gnn_base_model.train.training.load_loss_history">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">load_loss_history</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_nature</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load training process training loss history record.</span>
<span class="sd">    </span>
<span class="sd">    Loss history record file is stored in model_directory under the name</span>
<span class="sd">    loss_history_record.pkl.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model : torch.nn.Module</span>
<span class="sd">        Model.</span>
<span class="sd">    loss_nature : str</span>
<span class="sd">        Loss nature.</span>
<span class="sd">    loss_type : str</span>
<span class="sd">        Loss function type.</span>
<span class="sd">    epoch : int, default=None</span>
<span class="sd">        Epoch to which loss history is loaded (included), with the first epoch</span>
<span class="sd">        being 0. If None, then loads the full loss history.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    training_loss_history : list[float]</span>
<span class="sd">        Training process training loss history (per epoch).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set loss history record file path</span>
    <span class="n">loss_record_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span>
                                    <span class="s1">&#39;loss_history_record&#39;</span> <span class="o">+</span> <span class="s1">&#39;.pkl&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Load training process training loss history</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">):</span>
        <span class="c1"># Load loss history record</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">loss_record_file</span><span class="p">:</span>
            <span class="n">loss_history_record</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loss_record_file</span><span class="p">)</span>
        <span class="c1"># Check consistency between loss history nature and current training</span>
        <span class="c1"># process loss nature</span>
        <span class="n">history_loss_nature</span> <span class="o">=</span> <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;loss_nature&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">history_loss_nature</span> <span class="o">!=</span> <span class="n">loss_nature</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loss history nature (&#39;</span>
                               <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">history_loss_nature</span><span class="p">)</span>
                               <span class="o">+</span> <span class="s1">&#39;) is not consistent with current training &#39;</span>
                               <span class="s1">&#39;process loss nature (&#39;</span>
                               <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">loss_nature</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;).&#39;</span><span class="p">)</span>
        <span class="c1"># Check consistency between loss history type and current training</span>
        <span class="c1"># process loss type</span>
        <span class="n">history_loss_type</span> <span class="o">=</span> <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;loss_type&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">history_loss_type</span> <span class="o">!=</span> <span class="n">loss_type</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loss history type (&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">history_loss_type</span><span class="p">)</span>
                               <span class="o">+</span> <span class="s1">&#39;) is not consistent with current training &#39;</span>
                               <span class="s1">&#39;process loss type (&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">loss_type</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;).&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Check training loss history</span>
        <span class="n">loss_record</span> <span class="o">=</span> <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;training_loss_history&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss_record</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loaded loss history is not a list[float].&#39;</span><span class="p">)</span>
        <span class="c1"># Load training loss history</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_record</span><span class="p">):</span>
            <span class="n">training_loss_history</span> <span class="o">=</span> <span class="n">loss_record</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss_record</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Target epoch is beyond available loss &#39;</span>
                                   <span class="s1">&#39;history.&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">training_loss_history</span> <span class="o">=</span> <span class="n">loss_record</span><span class="p">[:</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Build training loss history with None entries if loss history record</span>
        <span class="c1"># file cannot be found</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Training process loss history file has not &#39;</span>
                               <span class="s1">&#39;been found and loaded epoch is unknown.&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">training_loss_history</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">[</span><span class="kc">None</span><span class="p">,]</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">training_loss_history</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="load_lr_history">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.load_lr_history.html#graphorge.gnn_base_model.train.training.load_lr_history">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">load_lr_history</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load training process learning rate history record.</span>
<span class="sd">    </span>
<span class="sd">    Loss history record file is stored in model_directory under the name</span>
<span class="sd">    loss_history_record.pkl.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model : torch.nn.Module</span>
<span class="sd">        Model.        </span>
<span class="sd">    epoch : int, default=None</span>
<span class="sd">        Training epoch to which loss history is loaded (included), with the</span>
<span class="sd">        first training epoch being 0. If None, then loads the full loss</span>
<span class="sd">        history.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    lr_history_epochs : list[float]</span>
<span class="sd">        Training process learning rate history (per epoch).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set loss history record file path</span>
    <span class="n">loss_record_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span>
                                    <span class="s1">&#39;loss_history_record&#39;</span> <span class="o">+</span> <span class="s1">&#39;.pkl&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Load training process learning history</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">):</span>
        <span class="c1"># Load loss history record</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">loss_record_file</span><span class="p">:</span>
            <span class="n">loss_history_record</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loss_record_file</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Check learning rate history</span>
        <span class="n">lr_record</span> <span class="o">=</span> <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;lr_history_epochs&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr_record</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="n">lr_record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loaded learning rate history is not a &#39;</span>
                               <span class="s1">&#39;list[float] or None.&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    </span>
        <span class="c1"># Load learning rate history</span>
        <span class="k">if</span> <span class="n">lr_record</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Build learning rate history with None entries if learning rate</span>
            <span class="c1"># history is not available</span>
            <span class="n">lr_history_epochs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
                <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;training_loss_history&#39;</span><span class="p">])</span><span class="o">*</span><span class="p">[</span><span class="kc">None</span><span class="p">,]</span>
        <span class="k">elif</span> <span class="n">lr_record</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Build learning rate history with None entries if learning rate</span>
            <span class="c1"># history is not available</span>
            <span class="n">lr_history_epochs</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">[</span><span class="kc">None</span><span class="p">,]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_record</span><span class="p">):</span>
                <span class="n">lr_history_epochs</span> <span class="o">=</span> <span class="n">lr_record</span>
            <span class="k">elif</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_record</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Target epoch is beyond available &#39;</span>
                                       <span class="s1">&#39;learning rate history.&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lr_history_epochs</span> <span class="o">=</span> <span class="n">lr_record</span><span class="p">[:</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Build learning rate history with None entries if loss history record</span>
        <span class="c1"># file cannot be found</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Training process loss history file has not &#39;</span>
                               <span class="s1">&#39;been found and loaded epoch is unknown.&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lr_history_epochs</span> <span class="o">=</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">[</span><span class="kc">None</span><span class="p">,]</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">lr_history_epochs</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="seed_worker">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.seed_worker.html#graphorge.gnn_base_model.train.training.seed_worker">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set workers seed in PyTorch data loaders to preserve reproducibility.</span>
<span class="sd">    </span>
<span class="sd">    Taken from: https://pytorch.org/docs/stable/notes/randomness.html</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    worker_id : int</span>
<span class="sd">        Worker ID.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="read_loss_history_from_file">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.read_loss_history_from_file.html#graphorge.gnn_base_model.train.training.read_loss_history_from_file">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">read_loss_history_from_file</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Read training process loss history from loss history record file.</span>
<span class="sd">    </span>
<span class="sd">    Loss history record file is stored in model_directory under the name</span>
<span class="sd">    loss_history_record.pkl.</span>
<span class="sd">    </span>
<span class="sd">    Detaches loss values from computation graph and moves them to CPU.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss_record_path : str</span>
<span class="sd">        Loss history record file path.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss_nature : str</span>
<span class="sd">        Loss nature.</span>
<span class="sd">    loss_type : str</span>
<span class="sd">        Loss function type.</span>
<span class="sd">    training_loss_history : list[float]</span>
<span class="sd">        Training process training loss history (per epoch).</span>
<span class="sd">    validation_loss_history : {None, list[float]}</span>
<span class="sd">        Training process validation loss history. Set to None if not available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check loss history record file</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loss history record file has not been found:</span><span class="se">\n\n</span><span class="s1">&#39;</span>
                           <span class="o">+</span> <span class="n">loss_record_path</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Load loss history record</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">loss_record_file</span><span class="p">:</span>
        <span class="n">loss_history_record</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loss_record_file</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Check loss history</span>
    <span class="k">if</span> <span class="s1">&#39;loss_nature&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">loss_history_record</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loss nature is not available in loss history &#39;</span>
                           <span class="s1">&#39;record.&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s1">&#39;loss_type&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">loss_history_record</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loss type is not available in loss history &#39;</span>
                           <span class="s1">&#39;record.&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s1">&#39;training_loss_history&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">loss_history_record</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loss history is not available in loss history &#39;</span>
                           <span class="s1">&#39;record.&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;training_loss_history&#39;</span><span class="p">],</span>
                        <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loss history is not a list[float].&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set loss nature</span>
    <span class="n">loss_nature</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;loss_nature&#39;</span><span class="p">])</span>
    <span class="c1"># Set loss type</span>
    <span class="n">loss_type</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;loss_type&#39;</span><span class="p">])</span>
    <span class="c1"># Set training loss history</span>
    <span class="n">training_loss_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;training_loss_history&#39;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">training_loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">training_loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set validation loss history</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;validation_loss_history&#39;</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">validation_loss_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;validation_loss_history&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">validation_loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">validation_loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">validation_loss_history</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">loss_nature</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">,</span> <span class="n">training_loss_history</span><span class="p">,</span>
            <span class="n">validation_loss_history</span><span class="p">)</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="read_lr_history_from_file">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.read_lr_history_from_file.html#graphorge.gnn_base_model.train.training.read_lr_history_from_file">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">read_lr_history_from_file</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Read training learning rate history from loss history record file.</span>
<span class="sd">    </span>
<span class="sd">    Loss history record file is stored in model_directory under the name</span>
<span class="sd">    loss_history_record.pkl.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss_record_path : str</span>
<span class="sd">        Loss history record file path.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    lr_scheduler_type : {&#39;steplr&#39;, &#39;explr&#39;, &#39;linlr&#39;}</span>
<span class="sd">        Type of learning rate scheduler:</span>

<span class="sd">        &#39;steplr&#39;  : Step-based decay (torch.optim.lr_scheduler.SetpLR)</span>
<span class="sd">        </span>
<span class="sd">        &#39;explr&#39;   : Exponential decay (torch.optim.lr_scheduler.ExponentialLR)</span>
<span class="sd">        </span>
<span class="sd">        &#39;linlr&#39;   : Linear decay (torch.optim.lr_scheduler.LinearLR)</span>

<span class="sd">    lr_history_epochs : list[float]</span>
<span class="sd">        Training process learning rate history (per epoch).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check loss history record file</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Loss history record file has not been found:</span><span class="se">\n\n</span><span class="s1">&#39;</span>
                           <span class="o">+</span> <span class="n">loss_record_path</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Load loss history record</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">loss_record_path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">loss_record_file</span><span class="p">:</span>
        <span class="n">loss_history_record</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loss_record_file</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Check learning rate history</span>
    <span class="k">if</span> <span class="s1">&#39;lr_scheduler_type&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">loss_history_record</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Learning rate scheduler type is not available in &#39;</span>
                           <span class="s1">&#39;loss history record.&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s1">&#39;lr_history_epochs&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">loss_history_record</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Learning rate history is not available in loss &#39;</span>
                           <span class="s1">&#39;history record.&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;lr_history_epochs&#39;</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Learning rate history is not a list[float].&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set learning rate scheduler type</span>
    <span class="n">lr_scheduler_type</span> <span class="o">=</span> <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;lr_scheduler_type&#39;</span><span class="p">]</span>
    <span class="c1"># Set learning rate history</span>
    <span class="n">lr_history_epochs</span> <span class="o">=</span> <span class="n">loss_history_record</span><span class="p">[</span><span class="s1">&#39;lr_history_epochs&#39;</span><span class="p">]</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">lr_scheduler_type</span><span class="p">,</span> <span class="n">lr_history_epochs</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="write_training_summary_file">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.write_training_summary_file.html#graphorge.gnn_base_model.train.training.write_training_summary_file">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">write_training_summary_file</span><span class="p">(</span>
    <span class="n">device_type</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">model_directory</span><span class="p">,</span> <span class="n">load_model_state</span><span class="p">,</span> <span class="n">n_max_epochs</span><span class="p">,</span>
    <span class="n">is_model_in_normalized</span><span class="p">,</span> <span class="n">is_model_out_normalized</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">is_sampler_shuffle</span><span class="p">,</span> <span class="n">loss_nature</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="p">,</span>
    <span class="n">opt_algorithm</span><span class="p">,</span> <span class="n">lr_init</span><span class="p">,</span> <span class="n">lr_scheduler_type</span><span class="p">,</span> <span class="n">lr_scheduler_kwargs</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span>
    <span class="n">dataset_file_path</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">best_loss</span><span class="p">,</span> <span class="n">best_training_epoch</span><span class="p">,</span> <span class="n">total_time_sec</span><span class="p">,</span>
    <span class="n">avg_time_epoch</span><span class="p">,</span> <span class="n">best_model_parameters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">torchinfo_summary</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Write summary data file for model training process.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    device_type : {&#39;cpu&#39;, &#39;cuda&#39;}</span>
<span class="sd">        Type of device on which torch.Tensor is allocated.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed used to initialize the random number generators of Python and</span>
<span class="sd">        other libraries (e.g., NumPy, PyTorch) for all devices to preserve</span>
<span class="sd">        reproducibility. Does also set workers seed in PyTorch data loaders.</span>
<span class="sd">    model_directory : str</span>
<span class="sd">        Directory where material patch model is stored.</span>
<span class="sd">    load_model_state : {&#39;best&#39;, &#39;last&#39;, int, None}</span>
<span class="sd">        Load available Graph Neural Network model state from the model</span>
<span class="sd">        directory. Data scalers are also loaded from model initialization file.</span>
<span class="sd">    n_max_epochs : int</span>
<span class="sd">        Maximum number of training epochs.</span>
<span class="sd">    is_model_in_normalized : bool, default=False</span>
<span class="sd">        If True, then model input features are assumed to be normalized</span>
<span class="sd">        (normalized input data has been seen during model training).</span>
<span class="sd">    is_model_out_normalized : bool, default=False</span>
<span class="sd">        If True, then model output features are assumed to be normalized</span>
<span class="sd">        (normalized output data has been seen during model training).</span>
<span class="sd">    batch_size : int</span>
<span class="sd">        Number of samples loaded per batch.</span>
<span class="sd">    is_sampler_shuffle : bool</span>
<span class="sd">        If True, shuffles data set samples at every epoch.</span>
<span class="sd">    loss_nature : str</span>
<span class="sd">        Loss nature.</span>
<span class="sd">    loss_type : str</span>
<span class="sd">        Loss function type.</span>
<span class="sd">    loss_kwargs : dict</span>
<span class="sd">        Arguments of torch.nn._Loss initializer.</span>
<span class="sd">    opt_algorithm : str</span>
<span class="sd">        Optimization algorithm.</span>
<span class="sd">    lr_init : float</span>
<span class="sd">        Initial value optimizer learning rate. Constant learning rate value if</span>
<span class="sd">        no learning rate scheduler is specified (lr_scheduler_type=None).</span>
<span class="sd">    lr_scheduler_type : str</span>
<span class="sd">        Type of learning rate scheduler.</span>
<span class="sd">    lr_scheduler_kwargs : dict</span>
<span class="sd">        Arguments of torch.optim.lr_scheduler.LRScheduler initializer.</span>
<span class="sd">    n_epochs : int</span>
<span class="sd">        Number of completed epochs in the training process.</span>
<span class="sd">    dataset_file_path : str</span>
<span class="sd">        Graph Neural Network graph data set file path if such file exists. Only</span>
<span class="sd">        used for output purposes.</span>
<span class="sd">    dataset : torch.utils.data.Dataset</span>
<span class="sd">        Graph Neural Network graph data set. Each sample corresponds to a</span>
<span class="sd">        torch_geometric.data.Data object describing a homogeneous graph.</span>
<span class="sd">    best_loss : float</span>
<span class="sd">        Best loss during training process.</span>
<span class="sd">    best_training_epoch : int</span>
<span class="sd">        Training epoch corresponding to best loss during training process.</span>
<span class="sd">    total_time_sec : int</span>
<span class="sd">        Total training time in seconds.</span>
<span class="sd">    avg_time_epoch : float</span>
<span class="sd">        Average training time per epoch.</span>
<span class="sd">    best_model_parameters : dict</span>
<span class="sd">        Model parameters corresponding to best model state.</span>
<span class="sd">    torchinfo_summary : str, default=None</span>
<span class="sd">        Torchinfo model architecture summary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set summary data</span>
    <span class="n">summary_data</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;device_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">device_type</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;seed&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">seed</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;model_directory&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_directory</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;load_model_state&#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="n">load_model_state</span> <span class="k">if</span> <span class="n">load_model_state</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;n_max_epochs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_max_epochs</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;is_model_in_normalized&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">is_model_in_normalized</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;is_model_out_normalized&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">is_model_out_normalized</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;is_sampler_shuffle&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">is_sampler_shuffle</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;loss_nature&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_nature</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;loss_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_type</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;loss_kwargs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_kwargs</span> <span class="k">if</span> <span class="n">loss_kwargs</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;opt_algorithm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">opt_algorithm</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;lr_init&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_init</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;lr_scheduler_type&#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="n">lr_scheduler_type</span> <span class="k">if</span> <span class="n">lr_scheduler_type</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;lr_scheduler_kwargs&#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="n">lr_scheduler_kwargs</span> <span class="k">if</span> <span class="n">lr_scheduler_kwargs</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;Number of completed epochs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_epochs</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;Training data set file&#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="n">dataset_file_path</span> <span class="k">if</span> <span class="n">dataset_file_path</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;Training data set size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;Best loss: &#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">best_loss</span><span class="si">:</span><span class="s1">.8e</span><span class="si">}</span><span class="s1"> (training epoch </span><span class="si">{</span><span class="n">best_training_epoch</span><span class="si">}</span><span class="s1">)&#39;</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;Total training time&#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">total_time_sec</span><span class="p">)))</span>
    <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;Avg. training time per epoch&#39;</span><span class="p">]</span> <span class="o">=</span> \
        <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">avg_time_epoch</span><span class="p">)))</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set summary optional data</span>
    <span class="k">if</span> <span class="n">best_model_parameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;Model parameters (best state)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_model_parameters</span>
    <span class="k">if</span> <span class="n">torchinfo_summary</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">summary_data</span><span class="p">[</span><span class="s1">&#39;torchinfo summary&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torchinfo_summary</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Write summary file</span>
    <span class="n">write_summary_file</span><span class="p">(</span>
        <span class="n">summary_directory</span><span class="o">=</span><span class="n">model_directory</span><span class="p">,</span>
        <span class="n">summary_title</span><span class="o">=</span><span class="s1">&#39;Summary: Model training&#39;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">summary_data</span><span class="p">)</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="EarlyStopper">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.EarlyStopper.html#graphorge.gnn_base_model.train.training.EarlyStopper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">EarlyStopper</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Early stopping procedure (implicit regularizaton).</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    _validation_size : float</span>
<span class="sd">        Size of the validation data set for early stopping evaluation, where</span>
<span class="sd">        size is a fraction of the whole data set contained between 0 and 1.</span>
<span class="sd">    _validation_frequency : int</span>
<span class="sd">        Frequency of validation procedures, i.e., frequency with respect to</span>
<span class="sd">        training epochs at which model is validated to evaluate early stopping</span>
<span class="sd">        criterion.</span>
<span class="sd">    _trigger_tolerance : int</span>
<span class="sd">        Number of consecutive model validation procedures without performance</span>
<span class="sd">        improvement to trigger early stopping.</span>
<span class="sd">    _improvement_tolerance : float</span>
<span class="sd">        Minimum relative improvement required to count as a performance</span>
<span class="sd">        improvement.</span>
<span class="sd">    _validation_steps_history : list</span>
<span class="sd">        Validation steps history.</span>
<span class="sd">    _validation_loss_history : list</span>
<span class="sd">        Validation loss history.</span>
<span class="sd">    _min_validation_loss : float</span>
<span class="sd">        Minimum validation loss.</span>
<span class="sd">    _n_not_improve : int</span>
<span class="sd">        Number of consecutive model validations without improvement.</span>
<span class="sd">    _best_model_state : dict</span>
<span class="sd">        Model state corresponding to the best performance.</span>
<span class="sd">    _best_optimizer_state : dict</span>
<span class="sd">        Optimizer state corresponding to the best performance.</span>
<span class="sd">    _best_training_epoch : int</span>
<span class="sd">        Training epoch corresponding to the best performance.</span>
<span class="sd">            </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    get_validation_loss_history(self)</span>
<span class="sd">        Get validation loss history.</span>
<span class="sd">    is_evaluate_criterion(self, epoch)</span>
<span class="sd">        Check whether to evaluate early stopping criterion.</span>
<span class="sd">    evaluate_criterion(self, model, optimizer, epoch, \</span>
<span class="sd">                       loss_nature=&#39;node_features_out&#39;, loss_type=&#39;mse&#39;, \</span>
<span class="sd">                       loss_kwargs={}, device_type=&#39;cpu&#39;)</span>
<span class="sd">        Evaluate early stopping criterion.</span>
<span class="sd">    _validate_model(self, model, optimizer, epoch,</span>
<span class="sd">                    loss_nature=&#39;node_features_out&#39;, loss_type=&#39;mse&#39;,</span>
<span class="sd">                    loss_kwargs={}, device_type=&#39;cpu&#39;)</span>
<span class="sd">        Perform model validation.</span>
<span class="sd">    load_best_performance_state(self, model, optimizer)</span>
<span class="sd">        Load minimum validation loss model and optimizer states.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="EarlyStopper.__init__">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.EarlyStopper.html#graphorge.gnn_base_model.train.training.EarlyStopper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">validation_frequency</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">trigger_tolerance</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">improvement_tolerance</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Constructor.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        validation_dataset : torch.utils.data.Dataset</span>
<span class="sd">            Graph Neural Network graph data set. Each sample corresponds to a</span>
<span class="sd">            torch_geometric.data.Data object describing a homogeneous graph.</span>
<span class="sd">        validation_frequency : int, default=1</span>
<span class="sd">            Frequency of validation procedures, i.e., frequency with respect to</span>
<span class="sd">            training epochs at which model is validated to evaluate early</span>
<span class="sd">            stopping criterion.</span>
<span class="sd">        trigger_tolerance : int, default=1</span>
<span class="sd">            Number of consecutive model validation procedures without</span>
<span class="sd">            performance improvement to trigger early stopping.</span>
<span class="sd">        improvement_tolerance : float, default=1e-2</span>
<span class="sd">            Minimum relative improvement required to count as a performance</span>
<span class="sd">            improvement.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set validation data set</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span>
        <span class="c1"># Set validation frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_frequency</span> <span class="o">=</span> <span class="n">validation_frequency</span>
        <span class="c1"># Set early stopping trigger tolerance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trigger_tolerance</span> <span class="o">=</span> <span class="n">trigger_tolerance</span>
        <span class="c1"># Set minimum relative improvement tolerance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_improvement_tolerance</span> <span class="o">=</span> <span class="n">improvement_tolerance</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize validation training steps history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_steps_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Initialize validation loss history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_loss_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Initialize minimum validation loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_min_validation_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="c1"># Initialize number of consecutive model validations without</span>
        <span class="c1"># improvement</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_not_improve</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize minimum validation loss state (best performance)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_best_model_state</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_best_optimizer_state</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_best_training_epoch</span> <span class="o">=</span> <span class="kc">None</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="EarlyStopper.get_validation_loss_history">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.EarlyStopper.html#graphorge.gnn_base_model.train.training.EarlyStopper.get_validation_loss_history">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_validation_loss_history</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get validation loss history.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        validation_loss_history : list[float]</span>
<span class="sd">            Validation loss history.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_loss_history</span><span class="p">)</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="EarlyStopper.is_evaluate_criterion">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.EarlyStopper.html#graphorge.gnn_base_model.train.training.EarlyStopper.is_evaluate_criterion">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_evaluate_criterion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check whether to evaluate early stopping criterion.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        epoch : int</span>
<span class="sd">            Training epoch.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        is_evaluate_criterion : bool</span>
<span class="sd">            If True, then early stopping criterion should be evaluated, False</span>
<span class="sd">            otherwise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_frequency</span> <span class="o">==</span> <span class="mi">0</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="EarlyStopper.evaluate_criterion">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.EarlyStopper.html#graphorge.gnn_base_model.train.training.EarlyStopper.evaluate_criterion">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate_criterion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span>
                           <span class="n">loss_nature</span><span class="o">=</span><span class="s1">&#39;node_features_out&#39;</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
                           <span class="n">loss_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate early stopping criterion.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : torch.nn.Module</span>
<span class="sd">            Graph Neural Network model.</span>
<span class="sd">        optimizer : torch.optim.Optimizer</span>
<span class="sd">            PyTorch optimizer.</span>
<span class="sd">        epoch : int</span>
<span class="sd">            Training epoch.</span>
<span class="sd">        loss_nature : {&#39;node_features_out&#39;, &#39;global_features_out&#39;}, \</span>
<span class="sd">                    default=&#39;node_features_out&#39;</span>
<span class="sd">            Loss nature:</span>
<span class="sd">            </span>
<span class="sd">            &#39;node_features_out&#39; : Based on node output features</span>

<span class="sd">            &#39;global_features_out&#39; : Based on global output features</span>

<span class="sd">        loss_type : {&#39;mse&#39;,}, default=&#39;mse&#39;</span>
<span class="sd">            Loss function type:</span>
<span class="sd">            </span>
<span class="sd">            &#39;mse&#39;  : MSE (torch.nn.MSELoss)</span>
<span class="sd">            </span>
<span class="sd">        loss_kwargs : dict, default={}</span>
<span class="sd">            Arguments of torch.nn._Loss initializer.</span>
<span class="sd">        batch_size : int, default=1</span>
<span class="sd">            Number of samples loaded per batch.</span>
<span class="sd">        device_type : {&#39;cpu&#39;, &#39;cuda&#39;}, default=&#39;cpu&#39;</span>
<span class="sd">            Type of device on which torch.Tensor is allocated.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        is_stop_training : bool</span>
<span class="sd">            True if early stopping criterion has been triggered, False</span>
<span class="sd">            otherwise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set early stopping flag</span>
        <span class="n">is_stop_training</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Perform model validation</span>
        <span class="n">avg_valid_loss_sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">loss_nature</span><span class="o">=</span><span class="n">loss_nature</span><span class="p">,</span>
            <span class="n">loss_type</span><span class="o">=</span><span class="n">loss_type</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="n">loss_kwargs</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Update minimum validation loss and performance counter</span>
        <span class="k">if</span> <span class="n">avg_valid_loss_sample</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_validation_loss</span><span class="p">:</span>
            <span class="c1"># Check relative performance improvement with respect to minimum</span>
            <span class="c1"># validation loss</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_steps_history</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Compute relative performance improvement</span>
                <span class="n">relative_improvement</span> <span class="o">=</span> \
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_min_validation_loss</span> <span class="o">-</span> <span class="n">avg_valid_loss_sample</span><span class="p">)</span><span class="o">/</span> \
                    <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_min_validation_loss</span><span class="p">)</span>
                <span class="c1"># Update performance counter</span>
                <span class="k">if</span> <span class="n">relative_improvement</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_improvement_tolerance</span><span class="p">:</span>
                    <span class="c1"># Reset performance counter (significant improvement)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_not_improve</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Reset performance counter (not significant improvement)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_not_improve</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Update minimum validation loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_validation_loss</span> <span class="o">=</span> <span class="n">avg_valid_loss_sample</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Save best performance state (minimum validation loss)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_best_model_state</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_best_optimizer_state</span> <span class="o">=</span> \
                <span class="nb">dict</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()),</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_best_training_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Increment performance counter</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_not_improve</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Evaluate early stopping criterion</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_not_improve</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trigger_tolerance</span><span class="p">:</span>
            <span class="c1"># Trigger early stopping</span>
            <span class="n">is_stop_training</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">return</span> <span class="n">is_stop_training</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="EarlyStopper._validate_model">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.EarlyStopper.html#graphorge.gnn_base_model.train.training.EarlyStopper._validate_model">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">_validate_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span>
                        <span class="n">loss_nature</span><span class="o">=</span><span class="s1">&#39;node_features_out&#39;</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
                        <span class="n">loss_kwargs</span><span class="o">=</span><span class="p">{},</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device_type</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform model validation.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : torch.nn.Module</span>
<span class="sd">            Graph Neural Network model.</span>
<span class="sd">        optimizer : torch.optim.Optimizer</span>
<span class="sd">            PyTorch optimizer.</span>
<span class="sd">        epoch : int</span>
<span class="sd">            Training epoch.</span>
<span class="sd">        loss_nature : {&#39;node_features_out&#39;, &#39;global_features_out&#39;}, \</span>
<span class="sd">                    default=&#39;node_features_out&#39;</span>
<span class="sd">            Loss nature:</span>
<span class="sd">            </span>
<span class="sd">            &#39;node_features_out&#39; : Based on node output features</span>

<span class="sd">            &#39;global_features_out&#39; : Based on global output features</span>

<span class="sd">        loss_type : {&#39;mse&#39;,}, default=&#39;mse&#39;</span>
<span class="sd">            Loss function type:</span>
<span class="sd">            </span>
<span class="sd">            &#39;mse&#39;  : MSE (torch.nn.MSELoss)</span>
<span class="sd">            </span>
<span class="sd">        loss_kwargs : dict, default={}</span>
<span class="sd">            Arguments of torch.nn._Loss initializer.</span>
<span class="sd">        batch_size : int, default=1</span>
<span class="sd">            Number of samples loaded per batch.</span>
<span class="sd">        device_type : {&#39;cpu&#39;, &#39;cuda&#39;}, default=&#39;cpu&#39;</span>
<span class="sd">            Type of device on which torch.Tensor is allocated.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        avg_predict_loss : float</span>
<span class="sd">            Average prediction loss per sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set material patch model state file name and path</span>
        <span class="n">model_state_file</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model_name</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
        <span class="c1"># Set material patch model state file path</span>
        <span class="n">model_state_path</span> <span class="o">=</span> \
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span> <span class="n">model_state_file</span> <span class="o">+</span> <span class="s1">&#39;.pt&#39;</span><span class="p">)</span>
        <span class="c1"># Set optimizer state file name and path</span>
        <span class="n">optimizer_state_file</span> <span class="o">=</span> \
            <span class="n">model</span><span class="o">.</span><span class="n">model_name</span> <span class="o">+</span> <span class="s1">&#39;_optim&#39;</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
        <span class="n">optimizer_state_path</span> <span class="o">=</span> \
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span> <span class="n">optimizer_state_file</span> <span class="o">+</span> <span class="s1">&#39;.pt&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize temporary state files flag</span>
        <span class="n">is_state_file_temp</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Save model and optimizer state files (required for validation)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">model_state_path</span><span class="p">):</span>
            <span class="c1"># Update temporary state files flag</span>
            <span class="n">is_state_file_temp</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># Save state files</span>
            <span class="n">save_training_state</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Prediction with Graph Neural Network model</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">avg_valid_loss_sample</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dataset</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">model_directory</span><span class="p">,</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">predict_directory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">load_model_state</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span>
            <span class="n">loss_nature</span><span class="o">=</span><span class="n">loss_nature</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="n">loss_type</span><span class="p">,</span>
            <span class="n">loss_kwargs</span><span class="o">=</span><span class="n">loss_kwargs</span><span class="p">,</span>
            <span class="n">is_normalized_loss</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">is_model_out_normalized</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">device_type</span><span class="o">=</span><span class="n">device_type</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set model in training mode</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Update validation epochs history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_steps_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="c1"># Propagate last validation loss until current epoch</span>
        <span class="n">history_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_loss_history</span><span class="p">)</span>
        <span class="n">history_gap</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">history_length</span>
        <span class="k">if</span> <span class="n">history_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validation_loss_history</span> <span class="o">+=</span> \
                <span class="n">history_gap</span><span class="o">*</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_loss_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],]</span>
        <span class="c1"># Append validation loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_valid_loss_sample</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Remove model and optimizer state files (required for validation)</span>
        <span class="k">if</span> <span class="n">is_state_file_temp</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">model_state_path</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">optimizer_state_path</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">return</span> <span class="n">avg_valid_loss_sample</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="EarlyStopper.load_best_performance_state">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.train.training.EarlyStopper.html#graphorge.gnn_base_model.train.training.EarlyStopper.load_best_performance_state">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_best_performance_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load minimum validation loss model and optimizer states.</span>
<span class="sd">        </span>
<span class="sd">        Both model and optimizer are updated &#39;in-place&#39; with stored state data.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model : torch.nn.Module</span>
<span class="sd">            Graph Neural Network model.</span>
<span class="sd">        optimizer : torch.optim.Optimizer</span>
<span class="sd">            PyTorch optimizer.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        best_training_epoch : int</span>
<span class="sd">            Training epoch corresponding to the best performance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check best performance states</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_best_model_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The best performance model state has not been &#39;</span>
                               <span class="s1">&#39;stored.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_best_optimizer_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The best performance optimization state has &#39;</span>
                               <span class="s1">&#39;not been stored.&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Load material patch model state</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_best_model_state</span><span class="p">)</span>
        <span class="c1"># Set loaded optimizer state</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_best_optimizer_state</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">])</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_best_training_epoch</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Bernardo Ferreira.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>