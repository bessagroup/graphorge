

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>graphorge.gnn_base_model.model.gnn_architectures &mdash; graphorge 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/readthedocs-custom.css?v=0fedbe66" />

  
      <script src="../../../../_static/documentation_options.js?v=8d563738"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            graphorge
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/getting_started/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/getting_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/getting_started/installation.html#package-manager">Package manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/getting_started/installation.html#from-source">From source</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../rst_doc_files/examples/example_workflow.html">Basic workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../_collections/examples/readme.html">Graphorge example directory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../_collections/examples/mechanics/gnn_shell_buckling/gnn_shell_buckling.html">A surrogate to estimate the knock-down factor of imperfect shells</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../_collections/examples/cfd/gnn_porous_medium/gnn_porous_medium.html">A surrogate to estimate the flow rate through a porous medium</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../_autosummary/graphorge.html">Code</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">License</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../license.html">MIT License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">graphorge</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">graphorge.gnn_base_model.model.gnn_architectures</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for graphorge.gnn_base_model.model.gnn_architectures</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Graph Neural Networks architectures.</span>

<span class="sd">Classes</span>
<span class="sd">-------</span>
<span class="sd">GraphIndependentNetwork(torch.nn.Module)</span>
<span class="sd">    Graph Independent Network.</span>
<span class="sd">GraphInteractionNetwork(torch_geometric.nn.MessagePassing)</span>
<span class="sd">    Graph Interaction Network.</span>
<span class="sd"> </span>
<span class="sd">Functions</span>
<span class="sd">---------</span>
<span class="sd">build_fnn</span>
<span class="sd">    Build multilayer feed-forward neural network.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1">#</span>
<span class="c1">#                                                                       Modules</span>
<span class="c1"># =============================================================================</span>
<span class="c1"># Third-party</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch_geometric.nn</span>
<span class="c1">#</span>
<span class="c1">#                                                          Authorship &amp; Credits</span>
<span class="c1"># =============================================================================</span>
<span class="n">__author__</span> <span class="o">=</span> <span class="s1">&#39;Bernardo Ferreira (bernardo_ferreira@brown.edu)&#39;</span>
<span class="n">__credits__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Bernardo Ferreira&#39;</span><span class="p">,</span> <span class="s1">&#39;Rui Barreira&#39;</span><span class="p">]</span>
<span class="n">__status__</span> <span class="o">=</span> <span class="s1">&#39;Planning&#39;</span>
<span class="c1"># =============================================================================</span>
<span class="c1">#</span>
<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="build_fnn">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.build_fnn.html#graphorge.gnn_base_model.model.gnn_architectures.build_fnn">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">build_fnn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
              <span class="n">output_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
              <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">[],</span>
              <span class="n">hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Build multilayer feed-forward neural network.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_size : int</span>
<span class="sd">        Number of neurons of input layer.</span>
<span class="sd">    output_size : int</span>
<span class="sd">        Number of neurons of output layer.</span>
<span class="sd">    output_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">        Output unit activation function. Defaults to identity (linear) unit</span>
<span class="sd">        activation function.</span>
<span class="sd">    hidden_layer_sizes : list[int], default=[]</span>
<span class="sd">        Number of neurons of hidden layers.</span>
<span class="sd">    hidden_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">        Hidden unit activation function. Defaults to identity (linear) unit</span>
<span class="sd">        activation function.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fnn : torch.nn.Sequential</span>
<span class="sd">        Multilayer feed-forward neural network.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check input and output size</span>
    <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="nb">int</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of input (</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span><span class="si">}</span><span class="s1">) and output &#39;</span>
                           <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s1">) features must be at least 1.&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set number of neurons of each layer</span>
    <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">layer_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
    <span class="n">layer_sizes</span> <span class="o">+=</span> <span class="n">hidden_layer_sizes</span>
    <span class="n">layer_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set number of layers of adaptive weights</span>
    <span class="n">n_layer</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set hidden and output layers unit activation functions</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">hidden_activation</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Hidden unit activation function must be derived &#39;</span>
                           <span class="s1">&#39;from torch.nn.Module class.&#39;</span><span class="p">)</span>
    <span class="n">activation_functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">hidden_activation</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layer</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">output_activation</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Output unit activation function must be derived &#39;</span>
                           <span class="s1">&#39;from torch.nn.Module class.&#39;</span><span class="p">)</span>
    <span class="n">activation_functions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_activation</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Create multilayer feed-forward neural network:</span>
    <span class="c1"># Initialize neural network</span>
    <span class="n">fnn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="c1"># Loop over neural network layers</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layer</span><span class="p">):</span>
        <span class="c1"># Set layer linear transformation</span>
        <span class="n">fnn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;Layer-&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                       <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                                       <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="c1"># Set layer unit activation function</span>
        <span class="n">fnn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;Activation-&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">activation_functions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">fnn</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="build_rnn">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.build_rnn.html#graphorge.gnn_base_model.model.gnn_architectures.build_rnn">[docs]</a>
<span class="k">def</span><span class="w"> </span><span class="nf">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_layer_sizes</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Build multilayer recurrent neural network.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_size : int</span>
<span class="sd">        Number of neurons of input layer.</span>
<span class="sd">    hidden_layer_sizes : list[int]</span>
<span class="sd">        Number of neurons of hidden layers.</span>
<span class="sd">    output_size : int</span>
<span class="sd">        Number of neurons of output layer.</span>
<span class="sd">    num_layers : list[int], default=None</span>
<span class="sd">        Number of layers in each RNN module. Defaults to None.</span>
<span class="sd">    rnn_cell : str, default=&#39;GRU&#39;</span>
<span class="sd">        RNN architecture cell.</span>
<span class="sd">    bias : bool, default=True</span>
<span class="sd">        Whether to use bias within the RNN cell.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    rnn : torch.nn.Sequential</span>
<span class="sd">        Multilayer recurrent neural network with linear output layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check input and output size</span>
    <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="nb">int</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of input (</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span><span class="si">}</span><span class="s1">) and output &#39;</span>
                           <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s1">) features must be at least 1.&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Check cells</span>
    <span class="k">if</span> <span class="n">rnn_cell</span> <span class="o">!=</span> <span class="s1">&#39;GRU&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">rnn_cell</span><span class="si">}</span><span class="s1">) is not a recognized RNN cell.&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">if</span> <span class="n">hidden_layer_sizes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># This ensures at least one pass through an RNN, before the linear</span>
        <span class="c1"># output layer.</span>
        <span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">output_size</span><span class="p">,</span> <span class="p">]</span>
    <span class="k">if</span> <span class="n">num_layers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="p">)):</span>
            <span class="n">num_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Expected same &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;number of &#39;&#39;num_layers&#39;&#39; as&#39;</span>
                               <span class="sa">f</span><span class="s1">&#39; &#39;&#39;hidden_layer_sizes&#39;&#39;. Instead, got &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span><span class="si">}</span><span class="s1"> and &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="p">)</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Set number of neurons of each layer</span>
    <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">layer_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
    <span class="n">layer_sizes</span> <span class="o">+=</span> <span class="n">hidden_layer_sizes</span>
    <span class="n">layer_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
    <span class="n">n_layer</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="c1"># Create multilayer recurrent neural network:</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="c1"># Loop over neural network layers</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layer</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Set layer linear transformation</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="n">rnn_cell</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span>
                       <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> 
                                    <span class="n">hidden_size</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
        <span class="c1"># Extracts &#39;output&#39; from &#39;(output, hidden_states)&#39;. &#39;hidden_state&#39; is </span>
        <span class="c1"># not passed between RNN cells, and it are internal variable to each</span>
        <span class="c1"># RNN-cell.</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN-output&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">),</span><span class="n">TorchRNNWrapper</span><span class="p">())</span>
    <span class="c1"># Linear output layer (last hidden_size -&gt; output_size)</span>
    <span class="n">rnn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;Output-layer&#39;</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                                                  <span class="n">out_features</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                                  <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">))</span>
    <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
    <span class="k">return</span> <span class="n">rnn</span></div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="TorchRNNWrapper">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.TorchRNNWrapper.html#graphorge.gnn_base_model.model.gnn_architectures.TorchRNNWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">TorchRNNWrapper</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TorchRNNWrapper.</span>
<span class="sd">    </span>
<span class="sd">    Extracts &#39;output&#39; features from the tuple &#39;(output, hidden_state)&#39; returned</span>
<span class="sd">    by the recurrent neural network architectures implemented in Pytorch.</span>
<span class="sd">    &#39;hidden_state&#39; is discarded.</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(self, x)</span>
<span class="sd">        Forward propagation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="TorchRNNWrapper.forward">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.TorchRNNWrapper.html#graphorge.gnn_base_model.model.gnn_architectures.TorchRNNWrapper.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward propagation.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : Tuple</span>
<span class="sd">            Tuple with (output, hidden_state) as returned by the recurrent </span>
<span class="sd">            neural network architectures implemented in Pytorch.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        output : {torch.Tensor, None}</span>
<span class="sd">            First element of the tuple, corresponding to the output of the </span>
<span class="sd">            recurrent neural network. </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">output</span></div>
</div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="GraphIndependentNetwork">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphIndependentNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphIndependentNetwork">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GraphIndependentNetwork</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Graph Independent Network.</span>
<span class="sd">    </span>
<span class="sd">    A Graph Network block with (1) distinct update functions for node, edge and</span>
<span class="sd">    global features implemented as multilayer feed-forward neural networks with</span>
<span class="sd">    layer normalization and (2) no aggregation functions, i.e., independent</span>
<span class="sd">    node, edges and global blocks.</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    _node_fn : torch.nn.Sequential</span>
<span class="sd">        Node update function.</span>
<span class="sd">    _n_node_in : int</span>
<span class="sd">        Number of node input features.</span>
<span class="sd">    _n_node_out : int</span>
<span class="sd">        Number of node output features.</span>
<span class="sd">    _edge_fn : torch.nn.Sequential</span>
<span class="sd">        Edge update function.</span>
<span class="sd">    _n_edge_in : int</span>
<span class="sd">        Number of edge input features.</span>
<span class="sd">    _n_edge_out : int</span>
<span class="sd">        Number of edge output features.</span>
<span class="sd">    _global_fn : torch.nn.Sequential</span>
<span class="sd">        Global update function.</span>
<span class="sd">    _n_global_in : int</span>
<span class="sd">        Number of global input features.</span>
<span class="sd">    _n_global_out : int</span>
<span class="sd">        Number of global output features.</span>
<span class="sd">    _n_time_node : int</span>
<span class="sd">        Number of discrete time steps of nodal features.</span>
<span class="sd">        If greater than 0, then nodal input features include a time</span>
<span class="sd">        dimension, and message passing layers are RNNs.</span>
<span class="sd">    _n_time_edge : int</span>
<span class="sd">        Number of discrete time steps of edge features.</span>
<span class="sd">        If greater than 0, then edge input features include a time </span>
<span class="sd">        dimension, and message passing layers are RNNs.</span>
<span class="sd">    _n_time_global : int</span>
<span class="sd">        Number of discrete time steps of global features.</span>
<span class="sd">        If greater than 0, then global input features include a time</span>
<span class="sd">        dimension, and message passing layers are RNNs.</span>
<span class="sd">    _is_norm_layer : bool, default=False</span>
<span class="sd">        If True, then add normalization layer to node, edge and global</span>
<span class="sd">        update functions.</span>
<span class="sd">    _is_skip_unset_update : bool</span>
<span class="sd">        If True, then return features input matrix when the corresponding</span>
<span class="sd">        update function has not been setup, otherwise return None.</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(self, node_features_in=None, edge_features_in=None, \</span>
<span class="sd">            global_features_in)</span>
<span class="sd">        Forward propagation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="GraphIndependentNetwork.__init__">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphIndependentNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphIndependentNetwork.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_hidden_layers</span><span class="p">,</span> <span class="n">hidden_layer_size</span><span class="p">,</span>
                 <span class="n">n_node_in</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_node_out</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_edge_in</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_edge_out</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_global_in</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_global_out</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_time_node</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_time_edge</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_time_global</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">node_hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">node_output_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">edge_hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">edge_output_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">global_hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">global_output_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">is_norm_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">is_skip_unset_update</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Constructor.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_hidden_layers : int</span>
<span class="sd">            Number of hidden layers of multilayer feed-forward neural network</span>
<span class="sd">            update functions.</span>
<span class="sd">        hidden_layer_size : int</span>
<span class="sd">            Number of neurons of hidden layers of multilayer feed-forward</span>
<span class="sd">            neural network update functions.</span>
<span class="sd">        n_node_in : int, default=0</span>
<span class="sd">            Number of node input features. Must be greater than zero to setup</span>
<span class="sd">            node update function.</span>
<span class="sd">        n_node_out : int, default=0</span>
<span class="sd">            Number of node output features. Must be greater than zero to setup</span>
<span class="sd">            node update function.</span>
<span class="sd">        n_edge_in : int, default=0</span>
<span class="sd">            Number of edge input features. Must be greater than zero to setup</span>
<span class="sd">            edge update function.</span>
<span class="sd">        n_edge_out : int, default=0</span>
<span class="sd">            Number of edge output features. Must be greater than zero to setup</span>
<span class="sd">            edge update function.</span>
<span class="sd">        n_global_in : int, default=0</span>
<span class="sd">            Number of global input features. Must be greater than zero to setup</span>
<span class="sd">            global update function.</span>
<span class="sd">        n_global_out : int, default=0</span>
<span class="sd">            Number of global output features. Must be greater than zero to</span>
<span class="sd">            setup global update function.</span>
<span class="sd">        node_hidden_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Hidden unit activation function of node update function (multilayer</span>
<span class="sd">            feed-forward neural network). Defaults to identity (linear) unit</span>
<span class="sd">            activation function.</span>
<span class="sd">        node_output_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Output unit activation function of node update function (multilayer</span>
<span class="sd">            feed-forward neural network). Defaults to identity (linear) unit</span>
<span class="sd">            activation function.</span>
<span class="sd">        edge_hidden_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Hidden unit activation function of edge update function (multilayer</span>
<span class="sd">            feed-forward neural network). Defaults to identity (linear) unit</span>
<span class="sd">            activation function.</span>
<span class="sd">        edge_output_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Output unit activation function of edge update function (multilayer</span>
<span class="sd">            feed-forward neural network). Defaults to identity (linear) unit</span>
<span class="sd">            activation function.</span>
<span class="sd">        global_hidden_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Hidden unit activation function of global update function</span>
<span class="sd">            (multilayer feed-forward neural network). Defaults to identity</span>
<span class="sd">            (linear) unit activation function.</span>
<span class="sd">        global_output_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Output unit activation function of global update function</span>
<span class="sd">            (multilayer feed-forward neural network). Defaults to identity</span>
<span class="sd">            (linear) unit activation function.</span>
<span class="sd">        n_time_node : int, default=0</span>
<span class="sd">            Number of discrete time steps of nodal features.</span>
<span class="sd">            If greater than 0, then nodal input features include a time </span>
<span class="sd">            dimension and message passing layers are RNNs.</span>
<span class="sd">        n_time_edge : int, default=0</span>
<span class="sd">            Number of discrete time steps of edge features.</span>
<span class="sd">            If greater than 0, then edge input features include a time</span>
<span class="sd">            dimension and message passing layers are RNNs.</span>
<span class="sd">        n_time_global : int, default=0</span>
<span class="sd">            Number of discrete time steps of global features.</span>
<span class="sd">            If greater than 0, then global input features include a time</span>
<span class="sd">            dimension and message passing layers are RNNs.</span>
<span class="sd">        is_norm_layer : bool, default=False</span>
<span class="sd">            If True, then add normalization layer to node, edge and global</span>
<span class="sd">            update functions.</span>
<span class="sd">        is_skip_unset_update : bool, default=False</span>
<span class="sd">            If True, then return features input matrix when the corresponding</span>
<span class="sd">            update function has not been setup, otherwise return None. Ignored</span>
<span class="sd">            if update function is setup.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Initialize Graph Network block from base class</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GraphIndependentNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set number of features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_node_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_node_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_edge_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_edge_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_global_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_global_out</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set input with time dimension</span>
        <span class="k">if</span> <span class="n">n_time_node</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n_time_edge</span> <span class="o">!=</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">n_time_node</span> <span class="o">!=</span> <span class="n">n_time_edge</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of time steps must match across &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;nodal and edge features. Instead, got &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_node=(</span><span class="si">{</span><span class="n">n_time_node</span><span class="si">}</span><span class="s1">) and &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_edge=(</span><span class="si">{</span><span class="n">n_time_edge</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_time_node</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n_time_global</span> <span class="o">!=</span><span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">n_time_node</span> <span class="o">!=</span> <span class="n">n_time_global</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of time steps must match across &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;nodal and global features. Instead, got &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_global=(</span><span class="si">{</span><span class="n">n_time_global</span><span class="si">}</span><span class="s1">) and &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_node=(</span><span class="si">{</span><span class="n">n_time_node</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_time_edge</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n_time_global</span> <span class="o">!=</span><span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">n_time_edge</span> <span class="o">!=</span> <span class="n">n_time_global</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of time steps must match across &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;edge and global features. Instead, got &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_global=(</span><span class="si">{</span><span class="n">n_time_global</span><span class="si">}</span><span class="s1">) and &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_edge=(</span><span class="si">{</span><span class="n">n_time_edge</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_time_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_time_edge</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_time_global</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set normalization layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_norm_layer</span> <span class="o">=</span> <span class="n">is_norm_layer</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set node update function as multilayer feed-forward neural network</span>
        <span class="c1"># or recurrent neural network</span>
        <span class="c1"># with layer normalization</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Set node update function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="p">,</span>
                                <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span>
                                <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                    <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="p">],</span>
                                <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span>
                                <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Build multilayer feed-forward neural network</span>
                <span class="n">fnn</span> <span class="o">=</span> <span class="n">build_fnn</span><span class="p">(</span>
                    <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="p">,</span>
                    <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="o">=</span><span class="n">node_output_activation</span><span class="p">,</span>
                    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span><span class="n">hidden_layer_size</span><span class="p">,],</span>
                    <span class="n">hidden_activation</span><span class="o">=</span><span class="n">node_hidden_activation</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;FNN&#39;</span><span class="p">,</span> <span class="n">fnn</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Add normalization layer (per-feature) to node update function</span>
            <span class="k">if</span> <span class="n">is_norm_layer</span><span class="p">:</span>
                <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span>
                    <span class="n">num_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;Norm-Layer&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set edge update function as multilayer feed-forward neural network</span>
        <span class="c1"># with layer normalization:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Set edge update function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="p">,</span>
                                <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span>
                                <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                    <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="p">],</span>
                                <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span>
                                <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Build multilayer feed-forward neural network</span>
                <span class="n">fnn</span> <span class="o">=</span> <span class="n">build_fnn</span><span class="p">(</span>
                    <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="p">,</span>
                    <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="o">=</span><span class="n">edge_output_activation</span><span class="p">,</span>
                    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span><span class="n">hidden_layer_size</span><span class="p">,],</span>
                    <span class="n">hidden_activation</span><span class="o">=</span><span class="n">edge_hidden_activation</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;FNN&#39;</span><span class="p">,</span> <span class="n">fnn</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Add normalization layer (per-feature) to edge update function</span>
            <span class="k">if</span> <span class="n">is_norm_layer</span><span class="p">:</span>
                <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span>
                    <span class="n">num_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;Norm-Layer&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span> <span class="o">=</span> <span class="kc">None</span>        
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set global update function as multilayer feed-forward neural network</span>
        <span class="c1"># with layer normalization:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Set global update function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="p">,</span>
                                <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span><span class="p">,</span>
                                <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                    <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="p">],</span>
                                <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Build multilayer feed-forward neural network</span>
                <span class="n">fnn</span> <span class="o">=</span> <span class="n">build_fnn</span><span class="p">(</span>
                    <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="p">,</span>
                    <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span><span class="p">,</span>
                    <span class="n">output_activation</span><span class="o">=</span><span class="n">global_output_activation</span><span class="p">,</span>
                    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span><span class="n">hidden_layer_size</span><span class="p">,],</span>
                    <span class="n">hidden_activation</span><span class="o">=</span><span class="n">global_hidden_activation</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;FNN&#39;</span><span class="p">,</span> <span class="n">fnn</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Add normalization layer (per-element) to global update function</span>
            <span class="k">if</span> <span class="n">is_norm_layer</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of global features &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="si">}</span><span class="s1">) must be &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;greater than 1 to compute standard &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;deviation in the corresponding &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;update function normalization &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;layer.&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span>
                        <span class="n">normalized_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span><span class="p">,</span>
                        <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;Norm-Layer&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Check update functions</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Graph Independent Network was initialized &#39;</span>
                               <span class="s1">&#39;without setting up any node, edge or global &#39;</span>
                               <span class="s1">&#39;update function. Set positive number of &#39;</span>
                               <span class="s1">&#39;features for at least the node, edge or &#39;</span>
                               <span class="s1">&#39;global update function.&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set flag to handle unset update function output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_skip_unset_update</span> <span class="o">=</span> <span class="n">is_skip_unset_update</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="GraphIndependentNetwork.forward">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphIndependentNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphIndependentNetwork.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">edge_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">global_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_vector</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward propagation.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node_features_in : torch.Tensor, default=None</span>
<span class="sd">            Nodes features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_nodes, n_features). Ignored if node update function is not</span>
<span class="sd">            setup.</span>
<span class="sd">        edge_features_in : torch.Tensor, default=None</span>
<span class="sd">            Edges features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_edges, n_features). Ignored if edge update function is not</span>
<span class="sd">            setup.</span>
<span class="sd">        global_features_in : torch.Tensor, default=None</span>
<span class="sd">            Global features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (1, n_features). Ignored if global update function is not setup.</span>
<span class="sd">        batch_vector : torch.Tensor, default=None</span>
<span class="sd">            Batch vector stored as torch.Tensor(1d) of shape (n_nodes,),</span>
<span class="sd">            assigning each node to a specific batch subgraph. Required to</span>
<span class="sd">            process a graph holding multiple isolated subgraphs when batch</span>
<span class="sd">            size is greater than 1.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        node_features_out : {torch.Tensor, None}</span>
<span class="sd">            Nodes features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_nodes, n_features).</span>
<span class="sd">        edge_features_out : {torch.Tensor, None}</span>
<span class="sd">            Edges features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_edges, n_features).</span>
<span class="sd">        global_features_out : {torch.Tensor, None}</span>
<span class="sd">            Global features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (1, n_features).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check number of nodes and nodes features</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node_features_in</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Nodes features input matrix is not a &#39;</span>
                                   <span class="s1">&#39;torch.Tensor.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_norm_layer</span> <span class="ow">and</span> <span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of nodes &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">) must be &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;greater than 1 to compute standard &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;deviation in the corresponding update &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;function normalization layer.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of node features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="si">}</span><span class="s1">) &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;and nodes input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of node features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="si">}</span><span class="s1">) and nodes &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
        <span class="c1"># Check number of edges and edges features</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge_features_in</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Edges features input matrix is not a &#39;</span>
                                   <span class="s1">&#39;torch.Tensor.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_norm_layer</span> <span class="ow">and</span> <span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of edges &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">) must be &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;greater than 1 to compute standard &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;deviation in the corresponding update &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;function normalization layer.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of edge features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="si">}</span><span class="s1">) &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;and edges input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of edge features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="si">}</span><span class="s1">) and edges &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
        <span class="c1"># Check number global features</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">global_features_in</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Global features input matrix is not a &#39;</span>
                                   <span class="s1">&#39;torch.Tensor.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of global features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model (&#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span><span class="si">}</span><span class="s1">) &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;and global input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of global features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="si">}</span><span class="s1">) and global &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Forward propagation: Node update function</span>
        <span class="n">node_features_out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">batch_size_node</span> <span class="o">=</span> <span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c1"># If we have time series data, reshape to a 3d tensor:</span>
                <span class="c1"># (n_time_node, batch_size, n_node_in)</span>
                <span class="n">node_features_in</span> <span class="o">=</span> \
                    <span class="n">node_features_in</span><span class="o">.</span><span class="n">view</span><span class="p">(</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">,</span> <span class="n">batch_size_node</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="c1"># Compute node update </span>
                <span class="n">node_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="p">(</span><span class="n">node_features_in</span><span class="p">)</span>
                <span class="c1"># If we have time series data, reshape back to 2d tensor:</span>
                <span class="c1"># (batch_size, n_node_in * n_time_node)</span>
                <span class="n">node_features_out</span> <span class="o">=</span> <span class="n">node_features_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size_node</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">node_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="p">(</span><span class="n">node_features_in</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_skip_unset_update</span><span class="p">:</span>
                <span class="n">node_features_out</span> <span class="o">=</span> <span class="n">node_features_in</span>       
        <span class="c1"># Forward propagation: Edge update function</span>
        <span class="n">edge_features_out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># If we have time series data, reshape to a 3d tensor:</span>
            <span class="c1"># (n_time_edge, batch_size, n_edge_in)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">batch_size_edge</span> <span class="o">=</span> <span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">edge_features_in</span> <span class="o">=</span> \
                    <span class="n">edge_features_in</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">,</span> <span class="n">batch_size_edge</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="c1"># Compute edge update</span>
                <span class="n">edge_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="p">(</span><span class="n">edge_features_in</span><span class="p">)</span>
                <span class="c1"># If we have time series data, reshape back to 2d tensor:</span>
                <span class="c1"># (batch_size, n_edge_in * n_time_edge)</span>
                <span class="n">edge_features_out</span> <span class="o">=</span> <span class="n">edge_features_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size_edge</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Compute edge update</span>
                <span class="n">edge_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="p">(</span><span class="n">edge_features_in</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_skip_unset_update</span><span class="p">:</span>
                <span class="n">edge_features_out</span> <span class="o">=</span> <span class="n">edge_features_in</span>
        <span class="c1"># Forward propagation: Global update function</span>
        <span class="n">global_features_out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># If we have time series data, reshape to a 3d tensor:</span>
            <span class="c1"># (n_time_global, batch_size, n_global_in)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">batch_size_global</span> <span class="o">=</span> <span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">global_features_in</span> <span class="o">=</span> \
                    <span class="n">global_features_in</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span><span class="p">,</span> <span class="n">batch_size_global</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="c1"># Compute global update</span>
                <span class="n">global_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="p">(</span><span class="n">global_features_in</span><span class="p">)</span>
                <span class="c1"># If we have time series data, reshape back to 2d tensor:</span>
                <span class="c1"># (batch_size, n_global_in * n_time_global)</span>
                <span class="n">global_features_out</span> <span class="o">=</span> \
                    <span class="n">global_features_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size_global</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Compute global update</span>
                <span class="n">global_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="p">(</span><span class="n">global_features_in</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_skip_unset_update</span><span class="p">:</span>
                <span class="n">global_features_out</span> <span class="o">=</span> <span class="n">global_features_in</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">return</span> <span class="n">node_features_out</span><span class="p">,</span> <span class="n">edge_features_out</span><span class="p">,</span> <span class="n">global_features_out</span></div>
</div>

<span class="c1"># =============================================================================</span>
<div class="viewcode-block" id="GraphInteractionNetwork">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GraphInteractionNetwork</span><span class="p">(</span><span class="n">torch_geometric</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MessagePassing</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Graph Interaction Network.</span>
<span class="sd">    </span>
<span class="sd">    A Graph Network block with (1) distinct update functions for node, edge and</span>
<span class="sd">    global features implemented as multilayer feed-forward or recurrent </span>
<span class="sd">    neural networks with layer normalization and (2) edge-to-node and </span>
<span class="sd">    node-to-global aggregation functions.</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    _node_fn : torch.nn.Sequential</span>
<span class="sd">        Node update function.</span>
<span class="sd">    _n_node_in : int</span>
<span class="sd">        Number of node input features.</span>
<span class="sd">    _n_node_out : int</span>
<span class="sd">        Number of node output features.</span>
<span class="sd">    _edge_fn : torch.nn.Sequential</span>
<span class="sd">        Edge update function.</span>
<span class="sd">    _n_edge_in : int</span>
<span class="sd">        Number of edge input features.</span>
<span class="sd">    _n_edge_out : int</span>
<span class="sd">        Number of edge input features.</span>
<span class="sd">    _global_fn : torch.nn.Sequential</span>
<span class="sd">        Global update function.</span>
<span class="sd">    _n_global_in : int</span>
<span class="sd">        Number of global input features.</span>
<span class="sd">    _n_global_out : int</span>
<span class="sd">        Number of global output features.</span>
<span class="sd">    _n_time_node : int</span>
<span class="sd">        Number of discrete time steps of nodal features.</span>
<span class="sd">        If greater than 0, then nodal input features include a time</span>
<span class="sd">        dimension, and message passing layers are RNNs.</span>
<span class="sd">    _n_time_edge : int</span>
<span class="sd">        Number of discrete time steps of edge features.</span>
<span class="sd">        If greater than 0, then edge input features include a time </span>
<span class="sd">        dimension, and message passing layers are RNNs.</span>
<span class="sd">    _n_time_global : int</span>
<span class="sd">        Number of discrete time steps of global features.</span>
<span class="sd">        If greater than 0, then global input features include a time</span>
<span class="sd">        dimension, and message passing layers are RNNs.</span>
<span class="sd">    _is_norm_layer : bool, default=False</span>
<span class="sd">        If True, then add normalization layer to node, edge and global</span>
<span class="sd">        update functions.</span>
<span class="sd">        </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(self, edges_indexes, node_features_in=None, edge_features_in=None)</span>
<span class="sd">        Forward propagation.</span>
<span class="sd">    message(self, node_features_in_i, node_features_in_j, \</span>
<span class="sd">            edge_features_in=None)</span>
<span class="sd">        Builds messages to node i from each edge (j, i) (edge update).</span>
<span class="sd">    update(self, node_features_in_aggr, node_features_in=None)</span>
<span class="sd">        Update node features.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="GraphInteractionNetwork.__init__">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_node_out</span><span class="p">,</span> <span class="n">n_edge_out</span><span class="p">,</span> <span class="n">n_hidden_layers</span><span class="p">,</span>
                 <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="n">n_node_in</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_edge_in</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_global_in</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">n_global_out</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_time_node</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_time_edge</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_time_global</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">edge_to_node_aggr</span><span class="o">=</span><span class="s1">&#39;add&#39;</span><span class="p">,</span> <span class="n">node_to_global_aggr</span><span class="o">=</span><span class="s1">&#39;add&#39;</span><span class="p">,</span>
                 <span class="n">node_hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">node_output_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">edge_hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">edge_output_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">global_hidden_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">global_output_activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
                 <span class="n">is_norm_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Constructor.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_node_out : int</span>
<span class="sd">            Number of node output features.</span>
<span class="sd">        n_edge_out : int</span>
<span class="sd">            Number of edge output features.</span>
<span class="sd">        n_hidden_layers : int</span>
<span class="sd">            Number of hidden layers of multilayer feed-forward neural network</span>
<span class="sd">            update functions.</span>
<span class="sd">        hidden_layer_size : int</span>
<span class="sd">            Number of neurons of hidden layers of multilayer feed-forward</span>
<span class="sd">            neural network update functions.</span>
<span class="sd">        n_node_in : int, default=0</span>
<span class="sd">            Number of node input features.</span>
<span class="sd">        n_edge_in : int, default=0</span>
<span class="sd">            Number of edge input features.</span>
<span class="sd">        n_global_in : int, default=0</span>
<span class="sd">            Number of global input features.</span>
<span class="sd">        n_global_out : int, default=0</span>
<span class="sd">            Number of global output features.</span>
<span class="sd">        edge_to_node_aggr : {&#39;add&#39;,}, default=&#39;add&#39;</span>
<span class="sd">            Edge-to-node aggregation scheme.</span>
<span class="sd">        node_to_global_aggr : {&#39;add&#39;, &#39;mean&#39;}, default=&#39;add&#39;</span>
<span class="sd">            Node-to-global aggregation scheme.</span>
<span class="sd">        node_hidden_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Hidden unit activation function of node update function (multilayer</span>
<span class="sd">            feed-forward neural network). Defaults to identity (linear) unit</span>
<span class="sd">            activation function.</span>
<span class="sd">        node_output_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Output unit activation function of node update function (multilayer</span>
<span class="sd">            feed-forward neural network). Defaults to identity (linear) unit</span>
<span class="sd">            activation function.</span>
<span class="sd">        edge_hidden_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Hidden unit activation function of edge update function (multilayer</span>
<span class="sd">            feed-forward neural network). Defaults to identity (linear) unit</span>
<span class="sd">            activation function.</span>
<span class="sd">        edge_output_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Output unit activation function of edge update function (multilayer</span>
<span class="sd">            feed-forward neural network). Defaults to identity (linear) unit</span>
<span class="sd">            activation function.</span>
<span class="sd">        global_hidden_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Hidden unit activation function of global update function</span>
<span class="sd">            (multilayer feed-forward neural network). Defaults to identity</span>
<span class="sd">            (linear) unit activation function.</span>
<span class="sd">        global_output_activation : torch.nn.Module, default=torch.nn.Identity</span>
<span class="sd">            Output unit activation function of global update function</span>
<span class="sd">            (multilayer feed-forward neural network). Defaults to identity</span>
<span class="sd">            (linear) unit activation function.</span>
<span class="sd">        n_time_node : int, default=0</span>
<span class="sd">            Number of discrete time steps of nodal features.</span>
<span class="sd">            If greater than 0, then nodal input features include a time </span>
<span class="sd">            dimension and message passing layers are RNNs.</span>
<span class="sd">        n_time_edge : int, default=0</span>
<span class="sd">            Number of discrete time steps of edge features.</span>
<span class="sd">            If greater than 0, then edge input features include a time</span>
<span class="sd">            dimension and message passing layers are RNNs.</span>
<span class="sd">        n_time_global : int, default=0</span>
<span class="sd">            Number of discrete time steps of global features.</span>
<span class="sd">            If greater than 0, then global input features include a time</span>
<span class="sd">            dimension and message passing layers are RNNs.</span>
<span class="sd">        is_norm_layer : bool, default=False</span>
<span class="sd">            If True, then add normalization layer to node, edge and global</span>
<span class="sd">            update functions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set aggregation scheme</span>
        <span class="k">if</span> <span class="n">edge_to_node_aggr</span> <span class="o">==</span> <span class="s1">&#39;add&#39;</span><span class="p">:</span>
            <span class="n">aggregation</span> <span class="o">=</span> <span class="n">torch_geometric</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">aggr</span><span class="o">.</span><span class="n">SumAggregation</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown aggregation scheme.&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set node-to-global aggregation scheme</span>
        <span class="k">if</span> <span class="n">node_to_global_aggr</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">node_to_global_aggr</span> <span class="o">=</span> <span class="n">node_to_global_aggr</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown node-to-global aggregation scheme.&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set flow direction of message passing</span>
        <span class="n">flow</span> <span class="o">=</span> <span class="s1">&#39;source_to_target&#39;</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize Graph Network block from base class</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GraphInteractionNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
                                                      <span class="n">flow</span><span class="o">=</span><span class="n">flow</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set number of features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_node_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_node_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_edge_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_edge_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_global_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_global_out</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set input with time dimension</span>
        <span class="k">if</span> <span class="n">n_time_node</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n_time_edge</span> <span class="o">!=</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">n_time_node</span> <span class="o">!=</span> <span class="n">n_time_edge</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of time steps must match across &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;nodal and edge features. Instead, got &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_node=(</span><span class="si">{</span><span class="n">n_time_node</span><span class="si">}</span><span class="s1">) and &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_edge=(</span><span class="si">{</span><span class="n">n_time_edge</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_time_node</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n_time_global</span> <span class="o">!=</span><span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">n_time_node</span> <span class="o">!=</span> <span class="n">n_time_global</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of time steps must match across &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;nodal and global features. Instead, got &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_global=(</span><span class="si">{</span><span class="n">n_time_global</span><span class="si">}</span><span class="s1">) and &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_node=(</span><span class="si">{</span><span class="n">n_time_node</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_time_edge</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">n_time_global</span> <span class="o">!=</span><span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">n_time_edge</span> <span class="o">!=</span> <span class="n">n_time_global</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of time steps must match across &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;edge and global features. Instead, got &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_global=(</span><span class="si">{</span><span class="n">n_time_global</span><span class="si">}</span><span class="s1">) and &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;n_time_edge=(</span><span class="si">{</span><span class="n">n_time_edge</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_time_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_time_edge</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_time_global</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set normalization layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_norm_layer</span> <span class="o">=</span> <span class="n">is_norm_layer</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Check number of input features</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span> <span class="o">&lt;</span> <span class="mi">1</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Impossible to setup model without node &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="si">}</span><span class="s1">), edge &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="si">}</span><span class="s1">), or global &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="si">}</span><span class="s1">) input features.&#39;</span><span class="p">)</span>
        <span class="c1"># Check number of output features</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of node (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="si">}</span><span class="s1">) and &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;edge (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="si">}</span><span class="s1">) output features &#39;</span>
                               <span class="sa">f</span><span class="s1">&#39;must be greater than 0.&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set node update function as multilayer feed-forward/recurrent neural</span>
        <span class="c1"># network with layer normalization:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Build recurrent neural network</span>
            <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span>
                            <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span>
                            <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="p">],</span>
                            <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span>
                            <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Build recurrent neural network</span>
                <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span>
                                <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span>
                                <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                    <span class="n">hidden_layer_size</span><span class="p">,</span> <span class="p">],</span>
                                <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span>
                                <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Build multilayer feed-forward neural network</span>
                <span class="n">fnn</span> <span class="o">=</span> <span class="n">build_fnn</span><span class="p">(</span>
                        <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span>
                        <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span>
                        <span class="n">output_activation</span><span class="o">=</span><span class="n">node_output_activation</span><span class="p">,</span>
                        <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                            <span class="n">hidden_layer_size</span><span class="p">,],</span>
                        <span class="n">hidden_activation</span><span class="o">=</span><span class="n">node_hidden_activation</span><span class="p">)</span>
                <span class="c1"># Set node update function</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;FNN&#39;</span><span class="p">,</span> <span class="n">fnn</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Add normalization layer (per-feature) to node update function</span>
        <span class="k">if</span> <span class="n">is_norm_layer</span><span class="p">:</span>
            <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span>
                <span class="n">num_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;Norm-Layer&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">)</span>        
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set edge update function as multilayer feed-forward or recurrent</span>
        <span class="c1"># neural network with layer normalization:</span>
        <span class="c1"># Set edge update function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Build recurrent neural network</span>
            <span class="c1"># regardless if sef._n_time_node &gt; 0 or not</span>
            <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="p">,</span>
                            <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span>
                            <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                <span class="n">hidden_layer_size</span><span class="p">,],</span>
                            <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span>
                            <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Build recurrent neural network</span>
                <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span> <span class="o">+</span>
                                    <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="p">,</span>
                                <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span>
                                <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                    <span class="n">hidden_layer_size</span><span class="p">,],</span>
                                <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span>
                                <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Build multilayer feed-forward neural network</span>
                <span class="n">fnn</span> <span class="o">=</span> <span class="n">build_fnn</span><span class="p">(</span>
                        <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="p">,</span>
                        <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span>
                        <span class="n">output_activation</span><span class="o">=</span><span class="n">edge_output_activation</span><span class="p">,</span>
                        <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                            <span class="n">hidden_layer_size</span><span class="p">,],</span>
                        <span class="n">hidden_activation</span><span class="o">=</span><span class="n">edge_hidden_activation</span><span class="p">)</span>
                <span class="c1"># Set edge update function</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;FNN&#39;</span><span class="p">,</span> <span class="n">fnn</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Add normalization layer (per-feature) to edge update function</span>
        <span class="k">if</span> <span class="n">is_norm_layer</span><span class="p">:</span>
            <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span>
                <span class="n">num_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_out</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;Norm-Layer&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Set global update function as multilayer feed-forward neural network</span>
        <span class="c1"># with layer normalization:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Set global update function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Build stacked recurrent neural network</span>
                <span class="c1"># regardless if sef._n_time_node &gt; 0 or self._n_time edge &gt; 0 </span>
                <span class="c1"># or not</span>
                <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span> <span class="o">+</span> 
                                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span>
                                <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span><span class="p">,</span>
                                <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                    <span class="n">hidden_layer_size</span><span class="p">,],</span>
                                <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span>
                                <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># Build stacked recurrent neural network</span>
                    <span class="n">rnn</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span> <span class="o">+</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span>
                                    <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span><span class="p">,</span>
                                    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                                        <span class="n">hidden_layer_size</span><span class="p">,],</span>
                                    <span class="n">rnn_cell</span><span class="o">=</span><span class="s1">&#39;GRU&#39;</span><span class="p">,</span>
                                    <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;RNN&#39;</span><span class="p">,</span> <span class="n">rnn</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Build multilayer feed-forward neural network</span>
                    <span class="n">fnn</span> <span class="o">=</span> <span class="n">build_fnn</span><span class="p">(</span>
                        <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_out</span><span class="p">,</span>
                        <span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span><span class="p">,</span>
                        <span class="n">output_activation</span><span class="o">=</span><span class="n">global_output_activation</span><span class="p">,</span>
                        <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">n_hidden_layers</span><span class="o">*</span><span class="p">[</span>
                            <span class="n">hidden_layer_size</span><span class="p">,],</span>
                        <span class="n">hidden_activation</span><span class="o">=</span><span class="n">global_hidden_activation</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;FNN&#39;</span><span class="p">,</span> <span class="n">fnn</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="c1"># Add normalization layer (per-element) to global update function</span>
            <span class="k">if</span> <span class="n">is_norm_layer</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of global features &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="si">}</span><span class="s1">) must be &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;greater than 1 to compute standard &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;deviation in the corresponding &#39;</span>
                                       <span class="sa">f</span><span class="s1">&#39;update function normalization layer.&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">norm_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span>
                        <span class="n">normalized_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_out</span><span class="p">,</span>
                        <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;Norm-Layer&#39;</span><span class="p">,</span> <span class="n">norm_layer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span> <span class="o">=</span> <span class="kc">None</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="GraphInteractionNetwork.forward">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">edges_indexes</span><span class="p">,</span> <span class="n">node_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">edge_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">global_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">batch_vector</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward propagation.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        edges_indexes : torch.Tensor</span>
<span class="sd">            Edges indexes matrix stored as torch.Tensor(2d) with shape</span>
<span class="sd">            (2, n_edges), where the i-th edge is stored in edges_indexes[:, i]</span>
<span class="sd">            as (start_node_index, end_node_index).</span>
<span class="sd">        node_features_in : torch.Tensor, default=None</span>
<span class="sd">            Nodes features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_nodes, n_features). If None, the edge-to-node aggregation is</span>
<span class="sd">            only built up to the highest receiver node index according with</span>
<span class="sd">            edges_indexes. To preserve total number of nodes in edge-to-node</span>
<span class="sd">            aggregation, pass torch.empty(n_nodes, 0) instead of None.</span>
<span class="sd">        edge_features_in : torch.Tensor, default=None</span>
<span class="sd">            Edges features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_edges, n_features).</span>
<span class="sd">        global_features_in : torch.Tensor, default=None</span>
<span class="sd">            Global features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (1, n_features). Ignored if global update function is not setup.</span>
<span class="sd">        batch_vector : torch.Tensor, default=None</span>
<span class="sd">            Batch vector stored as torch.Tensor(1d) of shape (n_nodes,),</span>
<span class="sd">            assigning each node to a specific batch subgraph. Required to</span>
<span class="sd">            process a graph holding multiple isolated subgraphs when batch</span>
<span class="sd">            size is greater than 1.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        node_features_out : torch.Tensor</span>
<span class="sd">            Nodes features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_nodes, n_features).</span>
<span class="sd">        edge_features_out : torch.Tensor</span>
<span class="sd">            Edges features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_edges, n_features).</span>
<span class="sd">        global_features_out : {torch.Tensor, None}</span>
<span class="sd">            Global features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (1, n_features).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check input features matrices</span>
        <span class="k">if</span> <span class="n">node_features_in</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">edge_features_in</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Impossible to compute forward propagation of &#39;</span>
                               <span class="s1">&#39;model without node (None) and edge (None) &#39;</span>
                               <span class="s1">&#39;input features matrices.&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Check edges indexes</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edges_indexes</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Edges indexes matrix is not a torch.Tensor.&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">edges_indexes</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">edges_indexes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Edges indexes matrix is not a torch.Tensor &#39;</span>
                               <span class="s1">&#39;of shape (2, n_edges).&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Check number of nodes and nodes features</span>
        <span class="k">if</span> <span class="n">node_features_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node_features_in</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Nodes features input matrix is not a &#39;</span>
                                   <span class="s1">&#39;torch.Tensor.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_norm_layer</span> <span class="ow">and</span> <span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of nodes &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">) must be &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;greater than 1 to compute standard &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;deviation in the corresponding update &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;functions normalization layer.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of node features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="si">}</span><span class="s1">) &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;and nodes input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of node features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_node_in</span><span class="si">}</span><span class="s1">) and nodes &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
        <span class="c1"># Check number of edges and edges features</span>
        <span class="k">if</span> <span class="n">edge_features_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edge_features_in</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Edges features input matrix is not a &#39;</span>
                                   <span class="s1">&#39;torch.Tensor.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_norm_layer</span> <span class="ow">and</span> <span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of edges &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">) must be &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;greater than 1 to compute standard &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;deviation in the corresponding update &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;function normalization layer.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">edges_indexes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of edges of graph &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;edges indexes (</span><span class="si">{</span><span class="n">edges_indexes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">) &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;and edges input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of edge features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="w"> </span><span class="si">}</span><span class="s1">) &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;and edges input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of edge features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_edge_in</span><span class="si">}</span><span class="s1">) and edges &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">edge_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
        <span class="c1"># Check global features</span>
        <span class="k">if</span> <span class="n">global_features_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">global_features_in</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Global features input matrix is not a &#39;</span>
                                   <span class="s1">&#39;torch.Tensor.&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of global features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span><span class="si">}</span><span class="s1">) &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;and global input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span> 
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> \
                <span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch of number of global features of &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;model (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_global_in</span><span class="si">}</span><span class="s1">) and global &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;input features matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">global_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span> 
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Perform graph neural network message-passing step (message,</span>
        <span class="c1"># aggregation, update) and get updated node features.</span>
        <span class="c1"># Time series data are reshaped within the message and update methods.</span>
        <span class="n">node_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span>
            <span class="n">edge_index</span><span class="o">=</span><span class="n">edges_indexes</span><span class="p">,</span> <span class="n">node_features_in</span><span class="o">=</span><span class="n">node_features_in</span><span class="p">,</span>
            <span class="n">edge_features_in</span><span class="o">=</span><span class="n">edge_features_in</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Get updated edge features</span>
        <span class="n">edge_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_features_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_edge_features_out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Initialize updated global features</span>
        <span class="n">global_features_out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Get update global features</span>
        <span class="c1"># Time series data are reshaped within the update_global method.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">global_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_global</span><span class="p">(</span>
                <span class="n">global_features_in</span><span class="o">=</span><span class="n">global_features_in</span><span class="p">,</span>
                <span class="n">node_features_out</span><span class="o">=</span><span class="n">node_features_out</span><span class="p">,</span>
                <span class="n">batch_vector</span><span class="o">=</span><span class="n">batch_vector</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">return</span> <span class="n">node_features_out</span><span class="p">,</span> <span class="n">edge_features_out</span><span class="p">,</span> <span class="n">global_features_out</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="GraphInteractionNetwork.message">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.message">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_features_in_i</span><span class="p">,</span> <span class="n">node_features_in_j</span><span class="p">,</span>
                <span class="n">edge_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Builds messages to node i from each edge (j, i) (edge update).</span>
<span class="sd">        </span>
<span class="sd">        Assumes that j is the source node and i is the receiver node (flow</span>
<span class="sd">        direction set as &#39;source_to_target&#39;). For each edge (j, i), the</span>
<span class="sd">        update function input features result from concatenation of the edge</span>
<span class="sd">        features and the corresponding nodes features.</span>
<span class="sd">        </span>
<span class="sd">        The source and receiver node input features mappings based on the edges</span>
<span class="sd">        indexes matrix are built in the _collect() method of class</span>
<span class="sd">        torch_geometric.nn.MessagePassing.</span>
<span class="sd">        </span>
<span class="sd">        The edges features output matrix is passed as the input tensor to the</span>
<span class="sd">        aggregation operator (class torch.nn.aggr.Aggregation) set in the</span>
<span class="sd">        initialization of the torch_geometric.nn.MessagePassing class.</span>

<span class="sd">        It is called by the &quot;propagate&quot; method within the PyG backend.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node_features_in_i : torch.Tensor</span>
<span class="sd">            Source node input features for each edge stored as a</span>
<span class="sd">            torch.Tensor(2d) of shape (n_edges, n_features). Mapping is</span>
<span class="sd">            performed based on the edges indexes matrix.</span>
<span class="sd">        node_features_in_j : torch.Tensor</span>
<span class="sd">            Receiver node input features for each edge stored as a</span>
<span class="sd">            torch.Tensor(2d) of shape (n_edges, n_features). Mapping is</span>
<span class="sd">            performed based on the edges indexes matrix.</span>
<span class="sd">        edge_features_in : torch.Tensor, default=None</span>
<span class="sd">            Edges features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_edges, n_features).</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        edge_features_out : torch.Tensor</span>
<span class="sd">            Edges features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_edges, n_features).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check input features</span>
        <span class="n">is_node_features_in</span> <span class="o">=</span> <span class="p">(</span><span class="n">node_features_in_i</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                               <span class="ow">and</span> <span class="n">node_features_in_j</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">is_edge_features_in</span> <span class="o">=</span> <span class="n">edge_features_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Concatenate available input features for each edge</span>
        <span class="k">if</span> <span class="n">is_node_features_in</span> <span class="ow">and</span> <span class="n">is_edge_features_in</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">edge_features_in</span> <span class="o">=</span> \
                    <span class="n">edge_features_in</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">node_features_in_i</span> <span class="o">=</span> \
                    <span class="n">node_features_in_i</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">)</span>
                <span class="n">node_features_in_j</span> <span class="o">=</span> \
                    <span class="n">node_features_in_j</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">)</span>
            <span class="c1"># Concatenate nodes and edges input features</span>
            <span class="n">edge_features_in_cat</span> <span class="o">=</span> \
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">node_features_in_i</span><span class="p">,</span> <span class="n">node_features_in_j</span><span class="p">,</span>
                        <span class="n">edge_features_in</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_node_features_in</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_edge_features_in</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">node_features_in_i</span> <span class="o">=</span> \
                    <span class="n">node_features_in_i</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">)</span>
                <span class="n">node_features_in_j</span> <span class="o">=</span> \
                    <span class="n">node_features_in_j</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">)</span>
            <span class="c1"># Concatenate nodes input features</span>
            <span class="n">edge_features_in_cat</span> <span class="o">=</span> \
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">node_features_in_i</span><span class="p">,</span> <span class="n">node_features_in_j</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">is_edge_features_in</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">edge_features_in</span> <span class="o">=</span> \
                    <span class="n">edge_features_in</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">)</span>
            <span class="c1"># Concatenate edges input features</span>
            <span class="n">edge_features_in_cat</span> <span class="o">=</span> <span class="n">edge_features_in</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Impossible to build edge update function &#39;</span>
                               <span class="s1">&#39;input features matrix without node (None) and &#39;</span>
                               <span class="s1">&#39;edge (None) input features matrices&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Update edge features</span>
        <span class="c1"># If we have time series data, reshape to a 3d tensor:</span>
        <span class="c1"># (n_time_edge, batch_size, n_edge_in)</span>
        <span class="c1"># If edges feature time series data, then it must be of the same size</span>
        <span class="c1"># as the time series data at the nodes</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">n_time</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">)</span>
            <span class="n">batch_size_edge</span> <span class="o">=</span> <span class="n">edge_features_in_cat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">edge_features_in_cat</span> <span class="o">=</span> \
                <span class="n">edge_features_in_cat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_time</span><span class="p">,</span> <span class="n">batch_size_edge</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Compute global update</span>
            <span class="n">edge_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="p">(</span><span class="n">edge_features_in_cat</span><span class="p">)</span>
            <span class="c1"># If we have time series data, reshape back to 2d tensor:</span>
            <span class="c1"># (batch_size, n_edge_in * n_time_edge)</span>
            <span class="n">edge_features_out</span> <span class="o">=</span> <span class="n">edge_features_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size_edge</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Compute global update</span>
            <span class="n">edge_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_fn</span><span class="p">(</span><span class="n">edge_features_in_cat</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Store updated edges features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_edge_features_out</span> <span class="o">=</span> <span class="n">edge_features_out</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">return</span> <span class="n">edge_features_out</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="GraphInteractionNetwork.update">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.update">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_features_in_aggr</span><span class="p">,</span> <span class="n">node_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update node features.</span>
<span class="sd">        </span>
<span class="sd">        The nodes features input matrix resulting from message passing and</span>
<span class="sd">        aggregation is built in the aggregation operator (class</span>
<span class="sd">        torch.nn.aggr.Aggregation) set in the initialization of the</span>
<span class="sd">        torch_geometric.nn.MessagePassing class.</span>

<span class="sd">        It is called by the propagate method within the PyG backend.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node_features_in_aggr : torch.Tensor</span>
<span class="sd">            Nodes features input matrix resulting from message passing and</span>
<span class="sd">            edge-to-node aggregation, stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_nodes, n_features).</span>
<span class="sd">        node_features_in : torch.Tensor, default=None</span>
<span class="sd">            Nodes features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_nodes, n_features).</span>
<span class="sd">              </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        node_features_out : torch.Tensor</span>
<span class="sd">            Nodes features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_nodes, n_features).</span>
<span class="sd">        &quot;&quot;&quot;</span>        
        <span class="c1"># Concatenate features for each node:</span>
        <span class="c1"># Set node features stemming from edge-to-node aggregation</span>
        <span class="c1"># It is not necessary to extend node_features_in_aggr along the last</span>
        <span class="c1"># dimension, as this was already done in the message() function, when </span>
        <span class="c1"># aggregating edge-to-nodal features. </span>
        <span class="c1"># Only node_features_in needs to be extended, for the case </span>
        <span class="c1"># self._n_time_edge &gt; 0 and self._n_time_node == 0, as </span>
        <span class="c1"># node_features_in_aggr will already be extended to include a time </span>
        <span class="c1"># dimension</span>
        <span class="n">node_features_in_cat</span> <span class="o">=</span> <span class="n">node_features_in_aggr</span>
        <span class="c1"># Concatenate available node input features</span>
        <span class="k">if</span> <span class="n">node_features_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Check number of nodes stemming from edge-to-node aggregation</span>
            <span class="k">if</span> <span class="n">node_features_in_aggr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Mismatch between number of nodes &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;stemming from edge-to-node aggregation &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">node_features_in_aggr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">) &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;and nodes features input matrix &#39;</span>
                                   <span class="sa">f</span><span class="s1">&#39;(</span><span class="si">{</span><span class="n">node_features_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">).&#39;</span><span class="p">)</span>
            <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">node_features_in</span> <span class="o">=</span> \
                    <span class="n">node_features_in</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">)</span>
            <span class="n">node_features_in_cat</span> <span class="o">=</span> \
                <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">node_features_in_cat</span><span class="p">,</span> <span class="n">node_features_in</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Update node features</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">n_time</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">)</span>
            <span class="c1"># If we have time series data, reshape to a 3d tensor:</span>
            <span class="c1"># (n_time_node, batch_size, n_node_in)</span>
            <span class="n">batch_size_node</span> <span class="o">=</span> <span class="n">node_features_in_cat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">node_features_in_cat</span> <span class="o">=</span> \
                <span class="n">node_features_in_cat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_time</span><span class="p">,</span> <span class="n">batch_size_node</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Compute global update</span>
            <span class="n">node_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="p">(</span><span class="n">node_features_in_cat</span><span class="p">)</span>
            <span class="c1"># If we have time series data, reshape back to 2d tensor:</span>
            <span class="c1"># (batch_size, n_node_in * n_time_node)</span>
            <span class="n">node_features_out</span> <span class="o">=</span> <span class="n">node_features_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size_node</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Compute global update</span>
            <span class="n">node_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_node_fn</span><span class="p">(</span><span class="n">node_features_in_cat</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">return</span> <span class="n">node_features_out</span></div>

    <span class="c1"># -------------------------------------------------------------------------</span>
<div class="viewcode-block" id="GraphInteractionNetwork.update_global">
<a class="viewcode-back" href="../../../../_autosummary/graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.html#graphorge.gnn_base_model.model.gnn_architectures.GraphInteractionNetwork.update_global">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_global</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_features_out</span><span class="p">,</span> <span class="n">global_features_in</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">batch_vector</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update global features.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        node_features_out : torch.Tensor</span>
<span class="sd">            Nodes features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (n_nodes, n_features).</span>
<span class="sd">            If self._n_time_node &gt; 0 or self._n_time_edge &gt; 0, then </span>
<span class="sd">            node_features_out has shape </span>
<span class="sd">            (n_nodes, n_features*self._n_time_node/edge).</span>
<span class="sd">        global_features_in : torch.Tensor, default=None</span>
<span class="sd">            Global features input matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (1, n_features).</span>
<span class="sd">        batch_vector : torch.Tensor, default=None</span>
<span class="sd">            Batch vector stored as torch.Tensor(1d) of shape (n_nodes,),</span>
<span class="sd">            assigning each node to a specific batch subgraph. Required to</span>
<span class="sd">            process a graph holding multiple isolated subgraphs when batch</span>
<span class="sd">            size is greater than 1.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        global_features_out : torch.Tensor</span>
<span class="sd">            Global features output matrix stored as a torch.Tensor(2d) of shape</span>
<span class="sd">            (1, n_features).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Perform node-to-global aggregation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_to_global_aggr</span> <span class="o">==</span> <span class="s1">&#39;add&#39;</span><span class="p">:</span>
            <span class="n">node_features_in_aggr</span> <span class="o">=</span> <span class="n">torch_geometric</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">global_add_pool</span><span class="p">(</span>
                <span class="n">node_features_out</span><span class="p">,</span> <span class="n">batch_vector</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">node_to_global_aggr</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="n">node_features_in_aggr</span> <span class="o">=</span> <span class="n">torch_geometric</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">global_mean_pool</span><span class="p">(</span>
                <span class="n">node_features_out</span><span class="p">,</span> <span class="n">batch_vector</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Unknown node-to-global aggregation scheme.&#39;</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Concatenate global features:</span>
        <span class="c1"># Set global features stemming from node-to-global aggregation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> \
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">global_features_in_cat</span> <span class="o">=</span> \
                <span class="n">node_features_in_aggr</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">global_features_in_cat</span> <span class="o">=</span> <span class="n">node_features_in_aggr</span>
        <span class="c1"># Concatenate available global input features</span>
        <span class="k">if</span> <span class="n">global_features_in</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># If (self._n_time_node &gt; 0 or self._n_time_edge &gt; 0) </span>
            <span class="c1"># and self._n_time_global == 0:</span>
            <span class="c1"># then global features must be extended with a time dimension</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> \
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">n_time_in</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">])</span>
                <span class="n">global_features_in</span> <span class="o">=</span> <span class="n">global_features_in</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_time_in</span><span class="p">)</span>
            <span class="n">global_features_in_cat</span> <span class="o">=</span> \
                    <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                        <span class="p">[</span><span class="n">global_features_in_cat</span><span class="p">,</span> <span class="n">global_features_in</span><span class="p">],</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="c1"># Update global features:</span>
        <span class="c1"># If we have time series data, reshape to a 3d tensor:</span>
        <span class="c1"># (n_time_global, batch_size, n_global_in)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">n_time</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_time_node</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_edge</span><span class="p">,</span> 
                          <span class="bp">self</span><span class="o">.</span><span class="n">_n_time_global</span><span class="p">])</span>
            <span class="n">batch_size_global</span> <span class="o">=</span> <span class="n">global_features_in_cat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">global_features_in_cat</span> <span class="o">=</span> \
                <span class="n">global_features_in_cat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n_time</span><span class="p">,</span> <span class="n">batch_size_global</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Compute global update</span>
            <span class="n">global_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="p">(</span><span class="n">global_features_in_cat</span><span class="p">)</span>
            <span class="c1"># If we have time series data, reshape back to 2d tensor:</span>
            <span class="c1"># (batch_size, n_global_in * n_time_node)</span>
            <span class="n">global_features_out</span> <span class="o">=</span> \
                <span class="n">global_features_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size_global</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">global_features_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_fn</span><span class="p">(</span><span class="n">global_features_in_cat</span><span class="p">)</span>
        <span class="c1"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
        <span class="k">return</span> <span class="n">global_features_out</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Bernardo Ferreira.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>